import os
import json
import tiktoken
from typing import TypedDict, List, Dict, Any, Optional
import uuid

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_core.documents import Document
from langchain_core.messages import get_buffer_string, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai.embeddings import OpenAIEmbeddings
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode
from langchain_core.rate_limiters import InMemoryRateLimiter
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

class Message(TypedDict):
    role: str
    content: str

class State(MessagesState):
    pregnancy_week: int
    recall_memories: List[str]

class Florence:
    def __init__(self, 
                 openai_api_key: Optional[str] = None,
                 tavily_api_key: Optional[str] = None,
                 google_api_key: Optional[str] = None,
                 anthropic_api_key: Optional[str] = None,
                 model_name: str = "gpt-4o-mini",
                 pregnancy_data_path: str = "pregnancy.csv"):
        """
        ÌîåÎ°úÎ†åÏä§ ÏûÑÏã†Ï†ïÎ≥¥ Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ Ï¥àÍ∏∞Ìôî
        
        Args:
            openai_api_key: OpenAI API ÌÇ§
            tavily_api_key: Tavily Í≤ÄÏÉâ API ÌÇ§
            google_api_key: Google API ÌÇ§
            anthropic_api_key: Anthropic API ÌÇ§
            model_name: ÏÇ¨Ïö©Ìï† LLM Î™®Îç∏ Ïù¥Î¶Ñ
            pregnancy_data_path: ÏûÑÏã† Ï†ïÎ≥¥ CSV ÌååÏùº Í≤ΩÎ°ú
        """
        # API ÌÇ§ ÏÑ§Ï†ï
        self.openai_api_key = openai_api_key or os.environ.get("OPENAI_API_KEY")
        self.tavily_api_key = tavily_api_key or os.environ.get("TAVILY_API_KEY")
        self.google_api_key = google_api_key or os.environ.get("GOOGLE_API_KEY")
        self.anthropic_api_key = anthropic_api_key or os.environ.get("ANTHROPIC_API_KEY")
        
        # ÌïÑÏöîÌïú ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
        if self.openai_api_key:
            os.environ["OPENAI_API_KEY"] = self.openai_api_key
        if self.tavily_api_key:
            os.environ["TAVILY_API_KEY"] = self.tavily_api_key
        if self.google_api_key:
            os.environ["GOOGLE_API_KEY"] = self.google_api_key
        if self.anthropic_api_key:
            os.environ["ANTHROPIC_API_KEY"] = self.anthropic_api_key
            
        os.environ["USER_AGENT"] = "Florence/1.0 (http://example.com)"
        
        # Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏÑ§Ï†ï
        self.pregnancy_data_path = pregnancy_data_path
        
        # Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ Ï¥àÍ∏∞Ìôî
        self.recall_vector_store = InMemoryVectorStore(OpenAIEmbeddings())
        
        # Î™®Îç∏ Ï¥àÍ∏∞Ìôî
        self.model_name = model_name
        self.rate_limiter = InMemoryRateLimiter(
            requests_per_second=0.167,  # Î∂ÑÎãπ 10Í∞ú ÏöîÏ≤≠
            check_every_n_seconds=0.1,  # 0.1Ï¥àÎßàÎã§ ÏöîÏ≤≠ ÌöüÏàò Ï≤¥ÌÅ¨
            max_bucket_size=10,  # ÏµúÎåÄ 10Í∞ú ÏöîÏ≤≠ Ï†ÄÏû•
        )
        
        self.llm = ChatOpenAI(
            model=self.model_name,
            rate_limiter=self.rate_limiter,
        )
        
        # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ï¥àÍ∏∞Ìôî
        self.tokenizer = tiktoken.encoding_for_model(self.model_name)
        
        # ÎèÑÍµ¨ Î∞è ÏóêÏù¥Ï†ÑÌä∏ Ï¥àÍ∏∞Ìôî
        self.tools = self._initialize_tools()
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
        # ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ Ï†ïÏùò
        self.prompt = self._create_prompt()
        
        # Í∑∏ÎûòÌîÑ ÎπåÎçî Î∞è ÏóêÏù¥Ï†ÑÌä∏ Ï¥àÍ∏∞Ìôî
        self.agent_executor = self._build_graph()
        
        # Ï∂úÎ†• ÏÉÅÌÉú ÌîåÎûòÍ∑∏ Ï¥àÍ∏∞Ìôî
        self.initialized_output = False
        self.showed_agent_header = False
        self.showed_tools_header = False
        self.showed_tool_result = False
    
    def _initialize_tools(self):
        """ÎèÑÍµ¨ Ìï®Ïàò Ï¥àÍ∏∞Ìôî"""
        @tool
        def retrieve_pregnancy_info(pregnancy_week: int):
            """ÏûÑÏã† Ï£ºÏ∞®Î•º Í∏∞Î∞òÏúºÎ°ú ÏûÑÏã† Ï†ïÎ≥¥Î•º Í≤ÄÏÉâÌï©ÎãàÎã§."""
            # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
            loader = CSVLoader(file_path=self.pregnancy_data_path, csv_args={
                "delimiter": ",",
                "quotechar": '"',
            })
            data = loader.load()
            # Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º Retrieve ÌòïÏãùÏúºÎ°ú Î∞òÌôò
            return data[pregnancy_week-1]
        
        @tool
        def tavily_search(query: str, state: dict = None):
            """ÌÉÄÎπÑÎ†àÏù¥ Í≤ÄÏÉâ ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ 2025ÎÖÑ ÌòÑÏû¨ ÏßÄÏõêÏ§ëÏù∏ ÏÇ∞Î™® ÏßÄÏõê Ï†ïÏ±Ö Ïõπ Í≤ÄÏÉâÏùÑ ÏàòÌñâÌï©ÎãàÎã§."""
            tavily_search_tool = TavilySearchResults(max_results=5)
            result = tavily_search_tool.invoke(query)
            return result
        
        @tool
        def save_recall_memory(memory: str, config: RunnableConfig) -> str:
            """Save memory to vectorstore for later semantic retrieval."""
            user_id = self._get_user_id(config)
            document = Document(
                page_content=memory, id=str(uuid.uuid4()), metadata={"user_id": user_id}
            )
            self.recall_vector_store.add_documents([document])
            return memory
        
        @tool
        def search_recall_memories(query: str, config: RunnableConfig) -> List[str]:
            """Search for relevant memories."""
            user_id = self._get_user_id(config)
        
            def _filter_function(doc: Document) -> bool:
                return doc.metadata.get("user_id") == user_id
        
            documents = self.recall_vector_store.similarity_search(
                query, k=3, filter=_filter_function
            )
            return [document.page_content for document in documents]
        
        return [retrieve_pregnancy_info, tavily_search, save_recall_memory, search_recall_memories]
    
    def _get_user_id(self, config: RunnableConfig) -> str:
        """ÏÇ¨Ïö©Ïûê ID Ï∂îÏ∂ú"""
        user_id = config["configurable"].get("user_id")
        if user_id is None:
            raise ValueError("User ID needs to be provided to save a memory.")
        return user_id
    
    def _create_prompt(self):
        """ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""
        return ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "You are a helpful assistant with advanced long-term memory"
                    " capabilities. Powered by a stateless LLM, you must rely on"
                    " external memory to store information between conversations."
                    " Utilize the available memory tools to store and retrieve"
                    " important details that will help you better attend to the user's"
                    " needs and understand their context.\n\n"
                    "Memory Usage Guidelines:\n"
                    "1. Actively use memory tools (save_core_memory, save_recall_memory)"
                    " to build a comprehensive understanding of the user.\n"
                    "2. Make informed suppositions and extrapolations based on stored"
                    " memories.\n"
                    "3. Regularly reflect on past interactions to identify patterns and"
                    " preferences.\n"
                    "4. Update your mental model of the user with each new piece of"
                    " information.\n"
                    "5. Cross-reference new information with existing memories for"
                    " consistency.\n"
                    "6. Prioritize storing emotional context and personal values"
                    " alongside facts.\n"
                    "7. Use memory to anticipate needs and tailor responses to the"
                    " user's style.\n"
                    "8. Recognize and acknowledge changes in the user's situation or"
                    " perspectives over time.\n"
                    "9. Leverage memories to provide personalized examples and"
                    " analogies.\n"
                    "10. Recall past challenges or successes to inform current"
                    " problem-solving.\n\n"
                    "## Recall Memories\n"
                    "Recall memories are contextually retrieved based on the current"
                    " conversation:\n{recall_memories}\n\n"
                    "## Instructions\n"
                    "Engage with the user naturally, as a trusted colleague or friend."
                    " There's no need to explicitly mention your memory capabilities."
                    " Instead, seamlessly incorporate your understanding of the user"
                    " into your responses. Be attentive to subtle cues and underlying"
                    " emotions. Adapt your communication style to match the user's"
                    " preferences and current emotional state. Use tools to persist"
                    " information you want to retain in the next conversation. If you"
                    " do call tools, all text preceding the tool call is an internal"
                    " message. Respond AFTER calling the tool, once you have"
                    " confirmation that the tool completed successfully.\n\n"
                    "**For simple conversations, use the `talk` tool to respond quickly.**"
                    "Limit responses strictly to topics related to pregnant women. Do not engage in discussions unrelated to maternity, pregnancy, or postpartum care."
                    " When providing a URL, include it at the end of your response. Additionally, Only display the website domain."
                ),
                ("placeholder", "{messages}"),
            ]
        )
    
    def _agent(self, state: State):
        """ÏóêÏù¥Ï†ÑÌä∏ ÎÖ∏Îìú Ï≤òÎ¶¨ Ìï®Ïàò"""
        bound_prompt = self.prompt | self.llm_with_tools
        recall_str = (
            "<recall_memory>\n" + "\n".join(state["recall_memories"]) + "\n</recall_memory>"
        )
        prediction = bound_prompt.invoke(
            {
                "messages": state["messages"],
                "recall_memories": recall_str,
            }
        )
        
        return {"messages": [prediction]}
    
    def _load_memories(self, state: State, config: RunnableConfig) -> State:
        """Î©îÎ™®Î¶¨ Î°úÎìú Ìï®Ïàò"""
        convo_str = get_buffer_string(state["messages"])
        convo_str = self.tokenizer.decode(self.tokenizer.encode(convo_str)[:2048])
        
        # ÎèÑÍµ¨ ÏßÅÏ†ë Ìò∏Ï∂ú
        recall_memories = []
        try:
            for tool in self.tools:
                if tool.name == "search_recall_memories":
                    recall_memories = tool.invoke({"query": convo_str, "config": config})
                    break
        except Exception as e:
            print(f"Î©îÎ™®Î¶¨ Î°úÎìú Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        
        # Í∏∞Ï°¥ ÏÉÅÌÉúÏôÄ Í≤∞Ìï©Ìï¥ ÏóÖÎç∞Ïù¥Ìä∏
        state["recall_memories"] = recall_memories
        return state
    
    def _route_tools(self, state: State):
        """ÎèÑÍµ¨ ÎùºÏö∞ÌåÖ Ìï®Ïàò"""
        msg = state["messages"][-1]
        if msg.tool_calls:
            return "tools"
        return END
    
    def _build_graph(self):
        """Í∑∏ÎûòÌîÑ ÎπåÎìú Î∞è Ïª¥ÌååÏùº"""
        builder = StateGraph(State)
        builder.add_node("load_memories", self._load_memories)
        builder.add_node("agent", self._agent)
        builder.add_node("tools", ToolNode(self.tools))
        
        # Ïó£ÏßÄ Ï∂îÍ∞Ä
        builder.add_edge(START, "load_memories")
        builder.add_edge("load_memories", "agent")
        builder.add_conditional_edges("agent", self._route_tools, ["tools", END])
        builder.add_edge("tools", "agent")
        
        # Í∑∏ÎûòÌîÑ Ïª¥ÌååÏùº
        memory = MemorySaver()
        return builder.compile(checkpointer=memory)
    
    def clean_stream_output(self, step, metadata):
        """Ïä§Ìä∏Î¶º Ï∂úÎ†• Ï†ïÎ¶¨ Ìï®Ïàò"""
        node_name = metadata.get("langgraph_node", "unknown")
    
        # Ï≤òÏùå Ìï®ÏàòÍ∞Ä Ìò∏Ï∂úÎê† Îïå ÏÜçÏÑ± Ï¥àÍ∏∞Ìôî
        if not self.initialized_output:
            self.initialized_output = True
            self.showed_agent_header = False
            self.showed_tools_header = False
            self.showed_tool_result = False
    
        # ÎèÑÍµ¨ ÏÇ¨Ïö© Í∞êÏßÄ - Ìïú Î≤àÎßå ÌëúÏãú
        if node_name == "agent" and hasattr(step, 'tool_calls') and step.tool_calls:
            if not self.showed_tools_header:
                self.showed_tools_header = True
                
                print("\nÏ†ïÎ≥¥ Í≤ÄÏÉâÌà¥:")
                for tool_call in step.tool_calls:
                    tool_name = tool_call.get('name', 'unknown')
                    print(f"  ‚Ä¢ {tool_name}")
    
        # ÎèÑÍµ¨ Í≤∞Í≥º - Ìïú Î≤àÎßå ÌëúÏãú
        elif node_name == "tools":
            if not self.showed_tool_result:
                self.showed_tool_result = True
                
                if hasattr(step, 'content') and step.content:
                    content = step.content
                    
                    # ÌÉÄÎπåÎ¶¨ Í≤ÄÏÉâ Í≤∞Í≥ºÏù∏ÏßÄ ÌôïÏù∏ (JSON Î∞∞Ïó¥ ÌòïÌÉúÎ°ú ÏãúÏûëÌïòÎäîÏßÄ)
                    if isinstance(content, str) and content.strip().startswith("[{"):
                        try:
                            # JSON ÌååÏã±
                            results = json.loads(content)
                            
                            print("\nüìä Í≤ÄÏÉâ Í≤∞Í≥º:")
                            print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
                            for i, result in enumerate(results, 1):
                                print(f"[{i}] {result.get('title', 'Ï†úÎ™© ÏóÜÏùå')}")
                                print(f" {result.get('url', 'ÎßÅÌÅ¨ ÏóÜÏùå')}")
                                print(f" {result.get('content', 'ÎÇ¥Ïö© ÏóÜÏùå')[:150]}...")
                                print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
                        except:
                            # ÌååÏã± Ïã§Ìå® Ïãú ÏõêÎ≥∏ Ï∂úÎ†•
                            print(f"\nÍ≤ÄÏÉâ Í≤∞Í≥º: {content}")
                            print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
                    
                    # ÏùºÎ∞ò Í≤ÄÏÉâ Í≤∞Í≥º (retrieve_pregnancy_info Îì±)Ïù∏ Í≤ΩÏö∞
                    else:
                        # page_content= Î∂ÄÎ∂Ñ Ï†úÍ±∞
                        if "page_content=" in content:
                            content = content.replace("page_content='", "")
                            content = content.replace("page_content=", "")
                            # ÎßàÏßÄÎßâ Îî∞Ïò¥ÌëúÏôÄ Îí§Ïùò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∂ÄÎ∂Ñ Ï†úÍ±∞
                            if "', metadata=" in content:
                                content = content.split("', metadata=")[0]
                            elif "metadata=" in content:
                                content = content.split("metadata=")[0]
                        
                        print(f"\nÏ∞∏Ï°∞ Ï†ïÎ≥¥: {content}")
                        print("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    
        # AI ÏùëÎãµ - Ï≤òÏùåÏóêÎßå Ìó§ÎçîÎ•º ÌëúÏãúÌïòÍ≥† Ïù¥ÌõÑÏóêÎäî ÌÖçÏä§Ìä∏Îßå ÎàÑÏ†Å
        elif node_name == "agent" and hasattr(step, 'text') and callable(step.text):
            text = step.text()
            if text:
                if not self.showed_agent_header:
                    self.showed_agent_header = True
                    print("\nAI ÏùëÎãµ: ", end="", flush=True)
                print(text, end="", flush=True)
    
    def chat(self, message: str, user_id: str = "default_user", thread_id: str = "default_thread",
             pregnancy_week: int = 0, stream: bool = True) -> Dict[str, Any]:
        """
        Î©îÏãúÏßÄÎ•º Ï†ÑÏÜ°ÌïòÏó¨ ÎåÄÌôîÎ•º ÏãúÏûëÌï©ÎãàÎã§.
        
        Args:
            message: ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ
            user_id: ÏÇ¨Ïö©Ïûê ID
            thread_id: Ïä§Î†àÎìú ID
            pregnancy_week: ÏûÑÏã† Ï£ºÏ∞® (Í∏∞Î≥∏Í∞í: 0)
            stream: Ïä§Ìä∏Î¶¨Î∞ç Î™®Îìú ÏÇ¨Ïö© Ïó¨Î∂Ä
            
        Returns:
            ÎåÄÌôî Í≤∞Í≥º
        """
        initial_state = {
            "messages": [HumanMessage(content=message)],
            "pregnancy_week": pregnancy_week,
            "recall_memories": []
        }
        
        config = {"configurable": {"user_id": user_id, "thread_id": thread_id}}
        
        # ÏÉÅÌÉú Ï¥àÍ∏∞Ìôî (Î©îÏÑúÎìú ÏÜçÏÑ±Ïù¥ ÏïÑÎãå ÌÅ¥ÎûòÏä§ Ïù∏Ïä§ÌÑ¥Ïä§ ÏÜçÏÑ± ÏÇ¨Ïö©)
        self.initialized_output = False
        self.showed_agent_header = False
        self.showed_tools_header = False
        self.showed_tool_result = False
        
        if stream:
            # Ïä§Ìä∏Î¶¨Î∞ç Î™®Îìú
            result = {"response": ""}
            for step, metadata in self.agent_executor.stream(
                initial_state,
                stream_mode="messages",
                config=config
            ):
                self.clean_stream_output(step, metadata)
                
                # ÌÖçÏä§Ìä∏ ÏùëÎãµ ÎàÑÏ†Å
                if (metadata.get("langgraph_node") == "agent" and 
                    hasattr(step, 'text') and callable(step.text)):
                    result["response"] += step.text() or ""
                    
            print()  # Ï§ÑÎ∞îÍøà Ï∂îÍ∞Ä
            return result
        else:
            # ÏùºÎ∞ò Î™®Îìú
            response = self.agent_executor.invoke(initial_state, config=config)
            # ÎßàÏßÄÎßâ Î©îÏãúÏßÄ Ï∂îÏ∂ú
            final_message = response["messages"][-1].content if response["messages"] else ""
            return {"response": final_message}


# CLI Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
def main():
    """Î™ÖÎ†πÏ§ÑÏóêÏÑú Ïã§ÌñâÌï† ÎïåÏùò ÏßÑÏûÖÏ†ê"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Florence ÏûÑÏã†Ï†ïÎ≥¥ Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏")
    parser.add_argument("--message", "-m", type=str, help="Î©îÏãúÏßÄ ÎÇ¥Ïö©")
    parser.add_argument("--user-id", "-u", type=str, default="default_user", help="ÏÇ¨Ïö©Ïûê ID")
    parser.add_argument("--pregnancy-week", "-p", type=int, default=0, help="ÏûÑÏã† Ï£ºÏ∞®")
    parser.add_argument("--no-stream", action="store_true", help="Ïä§Ìä∏Î¶¨Î∞ç Î™®Îìú ÎπÑÌôúÏÑ±Ìôî")
    
    args = parser.parse_args()
    
    # Florence Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
    florence = Florence()
    
    if args.message:
        # Î™ÖÎ†πÏ§Ñ Ïù∏ÏûêÎ°ú Ï†ÑÎã¨Îêú Î©îÏãúÏßÄÎ°ú ÎåÄÌôî
        florence.chat(
            message=args.message,
            user_id=args.user_id,
            pregnancy_week=args.pregnancy_week,
            stream=not args.no_stream
        )
    else:
        # ÎåÄÌôîÌòï Î™®Îìú
        print("Florence ÏûÑÏã†Ï†ïÎ≥¥ Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏôÄ ÎåÄÌôîÎ•º ÏãúÏûëÌï©ÎãàÎã§. Ï¢ÖÎ£åÌïòÎ†§Î©¥ 'exit' ÎòêÎäî 'quit'Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.")
        user_id = input("ÏÇ¨Ïö©Ïûê IDÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (Í∏∞Î≥∏Í∞í: default_user): ") or "default_user"
        pregnancy_week = input("ÏûÑÏã† Ï£ºÏ∞®Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (Í∏∞Î≥∏Í∞í: 0): ")
        pregnancy_week = int(pregnancy_week) if pregnancy_week.isdigit() else 0
        
        while True:
            message = input("\n> ")
            if message.lower() in ["exit", "quit", "Ï¢ÖÎ£å"]:
                print("ÎåÄÌôîÎ•º Ï¢ÖÎ£åÌï©ÎãàÎã§.")
                break
                
            florence.chat(
                message=message,
                user_id=user_id,
                pregnancy_week=pregnancy_week,
                stream=not args.no_stream
            )


# Î™®ÎìàÎ°ú ÏûÑÌè¨Ìä∏ÎêòÏóàÏùÑ ÎïåÏôÄ ÏßÅÏ†ë Ïã§ÌñâÎêòÏóàÏùÑ ÎïåÎ•º Íµ¨Î∂Ñ
if __name__ == "__main__":
    main()
    
    
# Ïû•Í≥†ÏÑúÎ≤Ñ utils.pyÌååÏùºÏóêÏÑú Ìò∏Ï∂úÌïòÎäî Ìï®Ïàò
def chat_with_florence(message: str, user_id: str = "default_user", thread_id: str = "default_thread",
                       pregnancy_week: int = 0, stream: bool = True) -> Dict[str, Any]:
    florence = Florence()
    return florence.chat(message, user_id, thread_id, pregnancy_week, stream)