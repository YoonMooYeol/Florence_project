from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status, viewsets
from rest_framework.parsers import JSONParser
from .method import RAGProcessor, RAGQuery
from .models import RAG_DB, RAG
from .serializers import RAGSerializer
from accounts.models import User
from dotenv import load_dotenv
import os
from tqdm import tqdm
import json
import time
import glob
import rdflib
from rdflib import URIRef

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# RAGProcessorì—ì„œ ì •ì˜ëœ ê²½ë¡œ ì‚¬ìš©
DB_DIR = RAGProcessor.DB_DIR
CSV_PATTERN = "data/rag/*.csv"

# DB_DIRì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
if not os.path.exists(DB_DIR):
    os.makedirs(DB_DIR, exist_ok=True)

class RAGViewSet(viewsets.ReadOnlyModelViewSet):
    """
    ì‚¬ìš©ìì˜ RAG ì¿¼ë¦¬ ê¸°ë¡ì„ ì¡°íšŒí•˜ëŠ” ë·°ì…‹
    """
    queryset = RAG.objects.all().order_by('-created_at')
    serializer_class = RAGSerializer
    
    def get_queryset(self):
        queryset = super().get_queryset()
        user_id = self.request.query_params.get('user_id')
        if user_id:
            queryset = queryset.filter(user__user_id=user_id)
        return queryset

class RAGQueryView(APIView):
    """
    RAG ì‹œìŠ¤í…œì— ì§ˆì˜í•˜ëŠ” API ë·°
    
    Endpoints:
        GET /rag/query/: API ì‚¬ìš© ë°©ë²• ë°˜í™˜
        POST /rag/query/: RAG ì‹œìŠ¤í…œì— ì§ˆì˜
    """
    parser_classes = (JSONParser,)
    permission_classes = []  # ì¸ì¦ ë¹„í™œì„±í™”
    
    def get(self, request):
        """API ì‚¬ìš© ë°©ë²•ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return Response({
            'message': 'RAG ì¿¼ë¦¬ API',
            'usage': {
                'method': 'POST',
                'description': 'RAG ì‹œìŠ¤í…œì— ì§ˆì˜í•©ë‹ˆë‹¤.',
                'endpoint': '/v1/rag/query/',
                'parameters': {
                    'query': '(í•„ìˆ˜) ì¿¼ë¦¬ í…ìŠ¤íŠ¸',
                    'max_tokens': '(ì„ íƒ) ì‘ë‹µ ìµœëŒ€ í† í° ìˆ˜',
                    'use_maternal_health': '(ì„ íƒ) ì‚°ëª¨ ê±´ê°• ë°ì´í„° ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: true)',
                    'user_id': '(ì„ íƒ) ì‚¬ìš©ì ID'
                }
            }
        }, status=status.HTTP_200_OK)
    
    def post(self, request):
        """RAG ì‹œìŠ¤í…œì— ì§ˆì˜í•©ë‹ˆë‹¤. ëª¨ë“  ì§ˆë¬¸ì€ ì‚°ëª¨ê°€ í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤."""
        try:
            query = request.data.get('query')
            max_tokens = request.data.get('max_tokens', 150)
            use_maternal_health = request.data.get('use_maternal_health', True)  # ê¸°ë³¸ê°’ì„ Trueë¡œ ë³€ê²½
            user_id = request.data.get('user_id')
            
            if not query:
                return Response({
                    'error': 'ì¿¼ë¦¬ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
                }, status=status.HTTP_400_BAD_REQUEST)
            
            # ëª¨ë“  ì§ˆë¬¸ì„ ì‚°ëª¨ ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
            retriever, chain = RAGQuery.create_maternal_health_qa_chain()
            retrieved_docs = retriever.invoke(query)
            retrieved_context = "\n".join([doc.page_content for doc in retrieved_docs])
            answer = chain.invoke({
                "retrieved_context": retrieved_context,
                "question": query
            }).content
            
            # ê²°ê³¼ ì €ì¥
            rag_entry = RAG(
                question=query,
                answer=answer
            )
            
            # ì‚¬ìš©ì IDê°€ ì œê³µëœ ê²½ìš° ì‚¬ìš©ì ì—°ê²°
            if user_id:
                try:
                    user = User.objects.get(user_id=user_id)
                    rag_entry.user = user
                except User.DoesNotExist:
                    pass  # ì‚¬ìš©ìê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° ë¬´ì‹œ
            
            rag_entry.save()
            
            return Response({
                'query': query,
                'answer': answer,
                'sources': [doc.metadata for doc in retrieved_docs],
                'processing_time': f"{time.time() - time.time():.2f}ì´ˆ"
            }, status=status.HTTP_200_OK)
            
        except Exception as e:
            return Response({
                'error': f'Error code: {e.__class__.__name__} - {str(e)}'
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class RAGAutoEmbedView(APIView):
    """
    data í´ë”ì˜ ëª¨ë“  íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì„ë² ë”©í•˜ëŠ” API ë·°
    
    Endpoints:
        GET /rag/auto-embed/: API ì‚¬ìš© ë°©ë²• ë°˜í™˜
        POST /rag/auto-embed/: ëª¨ë“  ë°ì´í„° íŒŒì¼ ìë™ ì„ë² ë”©
    """
    permission_classes = []  # ì¸ì¦ ë¹„í™œì„±í™”
    
    def get(self, request):
        """API ì‚¬ìš© ë°©ë²•ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return Response({
            'message': 'ìë™ ì„ë² ë”© API',
            'usage': {
                'method': 'POST',
                'description': 'data í´ë”ì˜ ëª¨ë“  íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤.',
                'endpoint': '/v1/rag/auto-embed/',
                'parameters': {
                    'data_dir': '(ì„ íƒ) ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ. ê¸°ë³¸ê°’ì€ "data"',
                    'file_types': '(ì„ íƒ) ì²˜ë¦¬í•  íŒŒì¼ ìœ í˜• ëª©ë¡. ê¸°ë³¸ê°’ì€ ["csv", "json", "rdf", "xml"]',
                    'force_reprocess': '(ì„ íƒ) ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ë„ ë‹¤ì‹œ ì²˜ë¦¬í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ false'
                }
            }
        }, status=status.HTTP_200_OK)
    
    def post(self, request):
        """ëª¨ë“  ë°ì´í„° íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        try:
            data_dir = request.data.get('data_dir', 'data')
            file_types = request.data.get('file_types', ['csv', 'json', 'rdf', 'xml'])
            force_reprocess = request.data.get('force_reprocess', False)
            
            # ê²°ê³¼ ì €ì¥ ë³€ìˆ˜
            results = {
                'total_files_found': 0,
                'new_files_processed': 0,
                'skipped_files': 0,
                'failed_files': 0,
                'total_documents_embedded': 0,
                'processing_time': 0,
                'file_details': []
            }
            
            # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ìƒì„±
            all_files = []
            for file_type in file_types:
                file_pattern = os.path.join(data_dir, f"**/*.{file_type}")
                files = glob.glob(file_pattern, recursive=True)
                for file in files:
                    all_files.append((file, file_type))
            
            results['total_files_found'] = len(all_files)
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            processed_files = set(RAG_DB.objects.values_list('file_path', flat=True))
            
            # ì‹œì‘ ì‹œê°„ ê¸°ë¡
            start_time = time.time()
            
            # íŒŒì¼ ì²˜ë¦¬
            with tqdm(total=len(all_files), desc="ğŸ“‚ ì „ì²´ íŒŒì¼ ì§„í–‰ë¥ ") as pbar:
                # ì´ˆê¸° ì§„í–‰ë¥  ë©”ì‹œì§€ ì„¤ì •
                pbar.set_postfix_str(f"ì²˜ë¦¬: 0ê°œ, ìŠ¤í‚µ: 0ê°œ")
                
                for file_path, file_type in all_files:
                    file_result = {
                        'file_path': file_path,
                        'file_type': file_type,
                        'status': '',
                        'documents_embedded': 0
                    }
                    
                    try:
                        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì¸ì§€ í™•ì¸
                        if file_path in processed_files and not force_reprocess:
                            file_result['status'] = 'skipped (already processed)'
                            results['skipped_files'] += 1
                            results['file_details'].append(file_result)
                            pbar.update(1)
                            # ìŠ¤í‚µëœ íŒŒì¼ í‘œì‹œ
                            pbar.set_postfix_str(f"ì²˜ë¦¬: {results['new_files_processed']}ê°œ, ìŠ¤í‚µ: {results['skipped_files']}ê°œ")
                            continue
                        
                        # force_reprocess=trueì´ê³  ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì¸ ê²½ìš°, ê¸°ì¡´ ë ˆì½”ë“œ ì‚­ì œ
                        if file_path in processed_files and force_reprocess:
                            RAG_DB.objects.filter(file_path=file_path).delete()
                        
                        # íŒŒì¼ í™•ì¥ì í™•ì¸
                        file_ext = os.path.splitext(file_path)[1].lower()
                        
                        # íŒŒì¼ ì´ë¦„ì—ì„œ ê±´ê°• ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                        file_name = os.path.basename(file_path).lower()
                        health_keywords = ['health', 'medical', 'wellness', 'ê±´ê°•', 'ì˜ë£Œ', 'ì›°ë‹ˆìŠ¤', 'ì‚°ëª¨', 'ì„ì‹ ', 'ì¶œì‚°', 'maternal', 'pregnancy']
                        is_health_related = any(keyword in file_name for keyword in health_keywords)
                        
                        if file_ext == '.csv':
                            # CSV íŒŒì¼ ì²˜ë¦¬
                            vectorstore, existing_ids = RAGProcessor.initialize_chroma_db()
                            
                            # CSV íŒŒì¼ ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
                            docs = RAGProcessor.load_csv_with_metadata(file_path)
                            if not docs:
                                raise ValueError("CSV íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            
                            # ìƒˆ ë¬¸ì„œ í•„í„°ë§
                            new_docs = RAGProcessor.filter_new_documents(docs, existing_ids, file_path)
                            
                            # ë¬¸ì„œ ë¶„í• 
                            splits = RAGProcessor.split_documents(new_docs)
                            
                            # ë°ì´í„° ì¤€ë¹„
                            texts, metadatas, ids = RAGProcessor.prepare_data_for_chroma(splits)
                            
                            # ì„ë² ë”© ìƒì„±
                            embeddings = RAGProcessor.create_embeddings(texts)
                            
                            # Chroma DB ì—…ë°ì´íŠ¸
                            RAGProcessor.update_chroma_db(
                                vectorstore, texts, embeddings, metadatas, ids, DB_DIR
                            )
                            
                            # ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
                            RAG_DB.objects.create(file_name=os.path.basename(file_path), file_path=file_path)
                            
                            file_result['status'] = 'success'
                            file_result['documents_embedded'] = len(new_docs)
                            results['total_documents_embedded'] += len(new_docs)
                            
                        elif file_ext == '.json':
                            # JSON íŒŒì¼ ì²˜ë¦¬
                            with open(file_path, "r", encoding="utf-8") as f:
                                file_content = f.read()
                            
                            try:
                                conversation = json.loads(file_content)
                            except Exception as e:
                                raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ JSON íŒŒì¼: {str(e)}")
                            
                            # í•„ìˆ˜ í‚¤ ì²´í¬: infoì™€ utterances í•„ìš”
                            if not {"info", "utterances"}.issubset(conversation.keys()):
                                raise ValueError("JSON íŒŒì¼ì€ infoì™€ utterances í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")
                            
                            # Chroma DB ì´ˆê¸°í™”
                            vectorstore, existing_ids = RAGProcessor.initialize_chroma_db()
                            
                            # JSON íŒŒì¼ ì²˜ë¦¬
                            vectorstore, new_docs, count = RAGProcessor.process_conversation_json(
                                conversation, existing_ids, vectorstore
                            )
                            
                            # ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
                            RAG_DB.objects.create(file_name=os.path.basename(file_path), file_path=file_path)
                            
                            file_result['status'] = 'success'
                            file_result['documents_embedded'] = new_docs
                            results['total_documents_embedded'] += new_docs
                            
                        elif file_ext in ['.rdf', '.xml', '.n-triples', '.nt']:
                            # RDF íŒŒì¼ ì²˜ë¦¬
                            # ê±´ê°• ê´€ë ¨ íŒŒì¼ì¸ ê²½ìš° ì‚°ëª¨ ê±´ê°• ë°ì´í„° ì²˜ë¦¬ ë°©ì‹ ì‚¬ìš©
                            if is_health_related:
                                print(f"ê±´ê°• ê´€ë ¨ ë°ì´í„°ë¡œ ì²˜ë¦¬: {file_path}")
                                # n-triples íŒŒì¼ì¸ ê²½ìš° ì§ì ‘ íŒŒì‹± ë°©ì‹ ì‚¬ìš©
                                if file_ext in ['.n-triples', '.nt']:
                                    result = RAGProcessor.embed_maternal_health_data_from_ntriples(file_path, format='nt')
                                else:
                                    result = RAGProcessor.embed_maternal_health_data(file_path)
                            else:
                                # ì¼ë°˜ RDF íŒŒì¼ ì²˜ë¦¬
                                result = RAGProcessor.process_rdf_data(file_path)
                            
                            # ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
                            RAG_DB.objects.create(file_name=os.path.basename(file_path), file_path=file_path)
                            
                            file_result['status'] = 'success'
                            file_result['documents_embedded'] = result.get('embedded_resources', 0)
                            results['total_documents_embedded'] += result.get('embedded_resources', 0)
                        
                        results['new_files_processed'] += 1
                        # ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜ í‘œì‹œ
                        pbar.set_postfix_str(f"ì²˜ë¦¬: {results['new_files_processed']}ê°œ, ìŠ¤í‚µ: {results['skipped_files']}ê°œ")
                        
                    except Exception as e:
                        file_result['status'] = 'failed'
                        file_result['error'] = str(e)
                        results['failed_files'] += 1
                        # ì‹¤íŒ¨í•œ íŒŒì¼ í‘œì‹œ
                        pbar.set_postfix_str(f"ì²˜ë¦¬: {results['new_files_processed']}ê°œ, ìŠ¤í‚µ: {results['skipped_files']}ê°œ, ì‹¤íŒ¨: {results['failed_files']}ê°œ")
                    
                    results['file_details'].append(file_result)
                    pbar.update(1)
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            end_time = time.time()
            results['processing_time'] = round(end_time - start_time, 2)
            
            # ê²°ê³¼ ìš”ì•½ ìƒì„±
            summary = {
                'message': 'ìë™ ì„ë² ë”© ì™„ë£Œ',
                'total_files_found': results['total_files_found'],
                'new_files_processed': results['new_files_processed'],
                'skipped_files': results['skipped_files'],
                'failed_files': results['failed_files'],
                'total_documents_embedded': results['total_documents_embedded'],
                'processing_time_seconds': results['processing_time'],
                'file_details': results['file_details']
            }
            
            return Response(summary, status=status.HTTP_200_OK)
            
        except Exception as e:
            return Response({
                'error': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

