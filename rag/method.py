import rdflib
from rdflib import Graph, URIRef, Literal
from rdflib.namespace import RDF, RDFS
import os
from django.conf import settings
from tqdm import tqdm
import json
from typing import List, Dict, Any, Tuple, Optional
import glob
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain_community.document_loaders import CSVLoader
import uuid
import pickle
from rag.models import RAG_DB
import asyncio
from langchain.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import re
import traceback
from langchain.schema import Document

# pandas ëª¨ë“ˆ import ì‹œë„
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("pandas ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ CSV ë¡œë”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

load_dotenv()

class RDFProcessor:
    """
    RDF ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ì¿¼ë¦¬í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, rdf_file_path="data/rdf/*.rdf", format=None):
        """
        RDF í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        
        Args:
            rdf_file_path (str, optional): RDF íŒŒì¼ ê²½ë¡œ. ê¸°ë³¸ê°’ì€ "data/rdf/*.rdf".
            format (str, optional): RDF íŒŒì¼ í˜•ì‹ ('nt', 'xml', 'turtle' ë“±). ê¸°ë³¸ê°’ì€ None(ìë™ ê°ì§€).
        """
        self.graph = rdflib.Graph()
        self.format = format
        
        # íŒŒì¼ ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ê·¸ë˜í”„ë¡œ ì´ˆê¸°í™”
        if not rdf_file_path:
            print("RDF íŒŒì¼ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¹ˆ ê·¸ë˜í”„ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            return
        
        self._load_files(rdf_file_path, format)
    
    def _load_files(self, file_path: str, format: Optional[str] = None) -> None:
        """
        íŒŒì¼ ê²½ë¡œì— ë”°ë¼ RDF íŒŒì¼ì„ ë¡œë“œ
        
        Args:
            file_path (str): RDF íŒŒì¼ ê²½ë¡œ (glob íŒ¨í„´ ê°€ëŠ¥)
            format (Optional[str]): RDF íŒŒì¼ í˜•ì‹
        """
        # glob íŒ¨í„´ì¸ ê²½ìš° ì²˜ë¦¬
        if '*' in file_path:
            matching_files = glob.glob(file_path)
            if not matching_files:
                print(f"ë§¤ì¹­ë˜ëŠ” RDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return
            
            # ëª¨ë“  ë§¤ì¹­ íŒŒì¼ ë¡œë“œ
            for file_path in matching_files:
                if os.path.exists(file_path):
                    file_format = self._detect_format(file_path, format)
                    self.load_rdf(file_path, file_format)
        elif os.path.exists(file_path):
            file_format = self._detect_format(file_path, format)
            self.load_rdf(file_path, file_format)
        else:
            print(f"RDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
    
    def _detect_format(self, file_path: str, format: Optional[str] = None) -> str:
        """
        íŒŒì¼ í™•ì¥ìì— ë”°ë¼ RDF í˜•ì‹ì„ ìë™ìœ¼ë¡œ ê°ì§€
        
        Args:
            file_path (str): RDF íŒŒì¼ ê²½ë¡œ
            format (Optional[str]): ì‚¬ìš©ìê°€ ì§€ì •í•œ í˜•ì‹ (ìˆëŠ” ê²½ìš°)
            
        Returns:
            str: ê°ì§€ëœ RDF í˜•ì‹
        """
        if format:
            return format
            
        if file_path.endswith('.rdf') or file_path.endswith('.xml'):
            return 'xml'
        elif file_path.endswith('.n-triples') or file_path.endswith('.nt'):
            return 'nt'
        elif file_path.endswith('.ttl'):
            return 'turtle'
        elif file_path.endswith('.jsonld'):
            return 'json-ld'
        else:
            # ê¸°ë³¸ê°’
            return 'xml'  # ê¸°ë³¸ê°’ì„ xmlë¡œ ë³€ê²½ (ì´ì „ì—ëŠ” nt)
    
    def load_rdf(self, file_path: str, format: Optional[str] = None) -> None:
        """
        RDF íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ê·¸ë˜í”„ì— ì¶”ê°€
        
        Args:
            file_path (str): RDF íŒŒì¼ ê²½ë¡œ
            format (Optional[str]): RDF íŒŒì¼ í˜•ì‹. ê¸°ë³¸ê°’ì€ None(ìë™ ê°ì§€).
        """
        try:
            # í˜•ì‹ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ê°ì§€
            if format is None:
                format = self._detect_format(file_path)
                
            print(f"RDF íŒŒì¼ ë¡œë“œ ì¤‘: {file_path} (í˜•ì‹: {format})")
            self.graph.parse(file_path, format=format)
            print(f"RDF ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {len(self.graph)} íŠ¸ë¦¬í”Œ")
        except Exception as e:
            print(f"RDF íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def execute_query(self, query: str) -> List[Dict[str, Any]]:
        """
        SPARQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜
        
        Args:
            query (str): SPARQL ì¿¼ë¦¬ ë¬¸ìì—´
            
        Returns:
            List[Dict[str, Any]]: ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        """
        try:
            results = self.graph.query(query)
            result_list = []
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            for row in results:
                row_dict = {}
                for var in results.vars:
                    value = row[var]
                    if isinstance(value, URIRef):
                        row_dict[var] = str(value)
                    elif isinstance(value, Literal):
                        row_dict[var] = value.value
                    else:
                        row_dict[var] = str(value)
                result_list.append(row_dict)
            
            return result_list
        except Exception as e:
            print(f"ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def get_resource_info(self, resource_uri: str) -> Dict[str, List[Any]]:
        """
        íŠ¹ì • ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ ëª¨ë“  ì†ì„±ê³¼ ê°’ì„ ê°€ì ¸ì˜´
        
        Args:
            resource_uri (str): ë¦¬ì†ŒìŠ¤ URI
            
        Returns:
            Dict[str, List[Any]]: ì†ì„±ì„ í‚¤ë¡œ, ê°’ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        """
        resource = URIRef(resource_uri)
        result = {}
        
        for s, p, o in self.graph.triples((resource, None, None)):
            pred = str(p)
            if pred not in result:
                result[pred] = []
            
            if isinstance(o, Literal):
                result[pred].append(o.value)
            else:
                result[pred].append(str(o))
        
        return result
    
    def get_resources_by_type(self, type_uri: str) -> List[str]:
        """
        íŠ¹ì • íƒ€ì…ì˜ ëª¨ë“  ë¦¬ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì˜´
        
        Args:
            type_uri (str): íƒ€ì… URI
            
        Returns:
            List[str]: ë¦¬ì†ŒìŠ¤ URI ë¦¬ìŠ¤íŠ¸
        """
        type_ref = URIRef(type_uri)
        resources = []
        
        for s in self.graph.subjects(RDF.type, type_ref):
            resources.append(str(s))
        
        return resources
    
    def get_label(self, uri: str, lang: str = "ko") -> Optional[str]:
        """
        URIì— ëŒ€í•œ ë¼ë²¨ì„ ê°€ì ¸ì˜´
        
        Args:
            uri (str): ë¦¬ì†ŒìŠ¤ URI
            lang (str, optional): ì–¸ì–´ ì½”ë“œ. ê¸°ë³¸ê°’ì€ 'ko'.
            
        Returns:
            Optional[str]: ë¼ë²¨ ë¬¸ìì—´ ë˜ëŠ” None
        """
        if not uri or not isinstance(uri, str):
            return None
            
        resource = URIRef(uri)
        for label in self.graph.objects(resource, RDFS.label):
            if isinstance(label, Literal) and (not lang or label.language == lang):
                return str(label)
        return None
    
    def convert_to_rag_format(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        RDF ì¿¼ë¦¬ ê²°ê³¼ë¥¼ RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            results (List[Dict[str, Any]]): RDF ì¿¼ë¦¬ ê²°ê³¼
            
        Returns:
            List[Dict[str, Any]]: RAG í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ ê²°ê³¼
        """
        rag_data = []
        
        for item in results:
            # ê° ë¦¬ì†ŒìŠ¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            content_parts = []
            
            # ë¦¬ì†ŒìŠ¤ URIì™€ ë¼ë²¨ ì¶”ê°€
            if 'uri' in item:
                content_parts.append(f"ë¦¬ì†ŒìŠ¤: {item['uri']}")
            if 'label' in item and item['label']:
                content_parts.append(f"ë¼ë²¨: {item['label']}")
            
            # ì†ì„± ì •ë³´ ì¶”ê°€
            if 'properties' in item:
                content_parts.append("ì†ì„±:")
                for prop, values in item['properties'].items():
                    if values:
                        values_str = ', '.join([str(v) for v in values])
                        content_parts.append(f"  - {prop}: {values_str}")
            
            # í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            content = '\n'.join(content_parts)
            
            # RAG í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            rag_item = {
                "content": content,
                "metadata": {
                    "source": "rdf",
                    "type": "knowledge_graph",
                    "uri": item.get('uri', ''),
                    "label": item.get('label', '')
                }
            }
            rag_data.append(rag_item)
        
        return rag_data
    
    def search_by_keyword(self, keyword: str, lang: str = "ko") -> List[Dict[str, Any]]:
        """
        í‚¤ì›Œë“œë¡œ ë¦¬ì†ŒìŠ¤ë¥¼ ê²€ìƒ‰
        
        Args:
            keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ
            lang (str, optional): ì–¸ì–´ ì½”ë“œ. ê¸°ë³¸ê°’ì€ 'ko'.
            
        Returns:
            List[Dict[str, Any]]: ê²€ìƒ‰ ê²°ê³¼
        """
        query = f"""
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        
        SELECT DISTINCT ?s ?label
        WHERE {{
            ?s rdfs:label ?label .
            FILTER(CONTAINS(STR(?label), "{keyword}"))
        }}
        LIMIT 100
        """
        
        results = self.execute_query(query)
        
        # ê° ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ ì¡°íšŒ
        enriched_results = []
        for result in results:
            if 's' in result:
                enriched_result = self._enrich_resource_info(result['s'], result.get('label', ''))
                enriched_results.append(enriched_result)
        
        return enriched_results
    
    def _enrich_resource_info(self, resource_uri: str, label: str = "") -> Dict[str, Any]:
        """
        ë¦¬ì†ŒìŠ¤ ì •ë³´ë¥¼ í’ë¶€í•˜ê²Œ ë§Œë“¤ì–´ ë°˜í™˜
        
        Args:
            resource_uri (str): ë¦¬ì†ŒìŠ¤ URI
            label (str, optional): ë¦¬ì†ŒìŠ¤ ë¼ë²¨
            
        Returns:
            Dict[str, Any]: í’ë¶€í•œ ë¦¬ì†ŒìŠ¤ ì •ë³´
        """
        resource_info = self.get_resource_info(resource_uri)
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ê°€
        enriched_result = {
            'uri': resource_uri,
            'label': label or self.get_label(resource_uri),
            'properties': {}
        }
        
        # ì†ì„± ì •ë³´ ì¶”ê°€
        for pred, values in resource_info.items():
            pred_label = self.get_label(pred) or pred
            enriched_result['properties'][pred_label] = []
            
            for value in values:
                if isinstance(value, str) and value.startswith('http'):
                    value_label = self.get_label(value)
                    if value_label:
                        enriched_result['properties'][pred_label].append(value_label)
                    else:
                        enriched_result['properties'][pred_label].append(value)
                else:
                    enriched_result['properties'][pred_label].append(value)
        
        return enriched_result

class RAGProcessor:
    """
    RAG ì²˜ë¦¬ í´ë˜ìŠ¤
    """
    TEMP_DIR = "data/temp_embeddings"
    DB_DIR = os.path.join(settings.BASE_DIR, "embeddings", "chroma_db")
    RDF_FILE_PATH = "data/rdf/wellness.rdf"
    
    # ì¤‘ìš” ì†ì„± ëª©ë¡ - ëª¨ë“  ì„ë² ë”© ê³¼ì •ì—ì„œ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•  ì†ì„±ë“¤
    IMPORTANT_PROPERTIES = [
        # ğŸš¨ ê¸ˆì§€ ë° ë¶€ì‘ìš© ê´€ë ¨
        "ê¸ˆì§€ì•½í’ˆ", "ê¸ˆì§€í•˜ë‹¤", "ë¶€ì‘ìš©", "forbidMedicine",  
        
        # ğŸ¥ ê±´ê°• ë° ì§ˆë³‘ ê´€ë ¨
        "ê±´ê°•", "ì§ˆë³‘", "ì¦ìƒ", "ì¹˜ë£Œ", "í•©ë³‘ì¦", "ë©´ì—­ë ¥", "ê°ì—¼", "ë°”ì´ëŸ¬ìŠ¤", 
        "ë…ê°", "ì½”ë¡œë‚˜", "í’ì§„", "í†¡ì†Œí”Œë¼ì¦ˆë§ˆ", "Bí˜•ê°„ì—¼", "Cí˜•ê°„ì—¼", 
        "ë¹ˆí˜ˆ", "ê°‘ìƒì„ ì§ˆí™˜", "ê³ í˜ˆì••", "ì €í˜ˆì••", "ë¹„ë§Œ", "ì²´ì¤‘ê°ì†Œ", "ìœ„ì¥ì¥ì• ", 
        "ìš”ë¡œê°ì—¼", "ë³€ë¹„", "relatedDisease",
        
        # ğŸ¤° ì„ì‹  ë° ì¶œì‚° ê´€ë ¨
        "ì„ì‹ ", "ì¶œì‚°", "ì‚°ëª¨", "íƒœì•„", "ì˜ì•„", "ì•„ê¸°", 
        "ì¶œì‚°ì¤€ë¹„", "ì¶œì‚°í›„ê´€ë¦¬", "ì‚°í›„ìš°ìš¸ì¦", "ì‚°í›„ì¡°ë¦¬", "ìœ ì‚°", "ì¡°ì‚°", 
        "ìì—°ë¶„ë§Œ", "ì œì™•ì ˆê°œ", "ì…ë§", "íƒœì•„ë°œë‹¬", "íƒœì•„ê±´ê°•", "ì–‘ìˆ˜", "íƒœë°˜", "íƒœë™", 
        "ì‚°í†µ", "bodyTransformation", "fetusDevelopment",
        
        # ğŸ½ ì˜ì–‘ ë° ì‹ìŠµê´€
        "ìŒì‹", "ì˜ì–‘", "ì‹ì´ìš”ë²•", "ì˜ì–‘ì†Œê²°í•", "ê³¼ì²´ì¤‘", "ì €ì²´ì¤‘", "ë‹¨ë°±ì§ˆì„­ì·¨", 
        "ì² ë¶„ì„­ì·¨", "ì¹¼ìŠ˜ì„­ì·¨", "ì—½ì‚°ì„­ì·¨", "ë¹„íƒ€ë¯¼Dì„­ì·¨", "ì„ì‚°ë¶€ìŒì‹", 
        "ê¶Œì¥ìŒì‹", "í”¼í•´ì•¼í• ìŒì‹", "ì¹´í˜ì¸ì„­ì·¨", "ì•Œì½”ì˜¬", "ê°€ê³µì‹í’ˆ", "í•´ì‚°ë¬¼", 
        "intakeNutrient", "recommendedFood", "avoidedFood",
        
        # ğŸƒâ€â™€ ìƒí™œ ìŠµê´€ ë° í™˜ê²½ ìš”ì¸
        "ìš´ë™", "ìš´ë™ë¶€ì¡±", "ê³¼ë¡œ", "ìì„¸", "ìì„¸êµì •", "ìŠ¤íŠ¸ë ˆìŠ¤", "ìŠ¤íŠ¸ë ˆìŠ¤í•´ì†Œ", 
        "ëª…ìƒ", "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "ìˆ˜ë©´", "ìˆ˜ë©´ë¶€ì¡±", "íœ´ì‹", 
        "ë¯¸ì„¸ë¨¼ì§€", "í™”í•™ë¬¼ì§ˆë…¸ì¶œ", "ì „ìíŒŒ", 
        
        # ğŸ§  ì •ì‹  ê±´ê°• ë° ì‹¬ë¦¬
        "ìš°ìš¸ì¦", "ë¶ˆì•ˆì¦", "ì •ì„œì•ˆì •", "ì‚°í›„ìŠ¤íŠ¸ë ˆìŠ¤", "ì‚¬íšŒì ì§€ì§€", "ë§ˆìŒì±™ê¹€", 
        "ìœ¡ì•„ìŠ¤íŠ¸ë ˆìŠ¤", "stressRelief"
    ]

    @staticmethod
    def parse_n_triples(file_path):
        """n-triples íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ì£¼ì œë³„ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤."""
        print(f"n-triples íŒŒì¼ íŒŒì‹± ì¤‘: {file_path}")
        
        # ì£¼ì œë³„ íŠ¸ë¦¬í”Œ ê·¸ë£¹í™”
        subjects = {}
        
        # ì„ì‹  ê´€ë ¨ í‚¤ì›Œë“œ
        pregnancy_keywords = [
            "pregnancy", "maternal", "ì„ì‹ ", "ì‚°ëª¨", "íƒœì•„", "ì¶œì‚°",
            "PregnancyPeriod", "FirstTrimester", "SecondTrimester", "ThirdTrimester"
        ]
        
        # n-triples íŒŒì¼ ì½ê¸°
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        print(f"ì´ {len(lines)}ê°œì˜ íŠ¸ë¦¬í”Œ ì½ê¸° ì™„ë£Œ")
        
        # íŠ¸ë¦¬í”Œ íŒŒì‹± ì •ê·œì‹
        triple_pattern = re.compile(r'<([^>]+)>\s+<([^>]+)>\s+(.+)\s+\.')
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        for line in tqdm(lines, desc="íŠ¸ë¦¬í”Œ íŒŒì‹±"):
            match = triple_pattern.match(line.strip())
            if match:
                subject, predicate, obj = match.groups()
                
                # ì£¼ì œê°€ subjects ë”•ì…”ë„ˆë¦¬ì— ì—†ìœ¼ë©´ ì¶”ê°€
                if subject not in subjects:
                    subjects[subject] = []
                
                # íŠ¸ë¦¬í”Œ ì¶”ê°€
                subjects[subject].append((predicate, obj))
        
        print(f"ì´ {len(subjects)}ê°œì˜ ì£¼ì œ ë°œê²¬")
        
        # ì„ì‹  ê´€ë ¨ ì£¼ì œ í•„í„°ë§
        pregnancy_subjects = {}
        for subject, triples in subjects.items():
            # ì£¼ì œ URIì— ì„ì‹  ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if any(keyword in subject.lower() for keyword in pregnancy_keywords):
                pregnancy_subjects[subject] = triples
                continue
            
            # íŠ¸ë¦¬í”Œì— ì„ì‹  ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            for predicate, obj in triples:
                if any(keyword in predicate.lower() for keyword in pregnancy_keywords) or \
                   any(keyword in obj.lower() for keyword in pregnancy_keywords):
                    pregnancy_subjects[subject] = triples
                    break
        
        print(f"ì„ì‹  ê´€ë ¨ ì£¼ì œ {len(pregnancy_subjects)}ê°œ ë°œê²¬")
        
        return pregnancy_subjects

    @staticmethod
    def create_embeddings_from_subjects(subjects):
        """ì£¼ì œë³„ íŠ¸ë¦¬í”Œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì„ë² ë”©í•©ë‹ˆë‹¤."""
        texts = []
        metadatas = []
        ids = []
        
        # ì¤‘ìš” ì†ì„± ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        important_props = RAGProcessor.IMPORTANT_PROPERTIES
        
        # ì£¼ì œë³„ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
        for idx, (subject, triples) in enumerate(subjects.items()):
            # í…ìŠ¤íŠ¸ ìƒì„±
            content_parts = [f"ì£¼ì œ: {subject}"]
            
            # íŠ¸ë¦¬í”Œ ì •ë³´ ì¶”ê°€
            predicates = {}
            important_attrs = {}
            
            for predicate, obj in triples:
                if predicate not in predicates:
                    predicates[predicate] = []
                
                # ê°ì²´ê°€ URIì¸ ê²½ìš° < > ì œê±°
                if obj.startswith('<') and obj.endswith('>'):
                    obj = obj[1:-1]
                # ê°ì²´ê°€ ë¦¬í„°ëŸ´ì¸ ê²½ìš° ë”°ì˜´í‘œì™€ ì–¸ì–´ íƒœê·¸ ì œê±°
                elif obj.startswith('"') and obj.endswith('"'):
                    obj = obj[1:-1]
                elif obj.startswith('"') and '"^^' in obj:
                    obj = obj[1:obj.find('"^^')]
                elif obj.startswith('"') and '"@' in obj:
                    obj = obj[1:obj.find('"@')]
                
                # ì¤‘ìš” ì†ì„± í™•ì¸
                for prop in important_props:
                    if prop in predicate.lower() or prop in obj.lower():
                        # ì†ì„± ì´ë¦„ ì¶”ì¶œ (URIì˜ ë§ˆì§€ë§‰ ë¶€ë¶„)
                        attr_name = predicate.split('/')[-1].split('#')[-1]
                        important_attrs[attr_name] = obj
                
                predicates[predicate].append(obj)
            
            # ì¤‘ìš” ì†ì„± ë¨¼ì € ì¶”ê°€
            for prop in important_props:
                for pred, values in predicates.items():
                    if prop in pred:
                        values_str = ', '.join(values)
                        content_parts.append(f"{pred}: {values_str}")
            
            # ë‚˜ë¨¸ì§€ ì†ì„± ì¶”ê°€
            for pred, values in predicates.items():
                if not any(prop in pred for prop in important_props):
                    values_str = ', '.join(values)
                    content_parts.append(f"{pred}: {values_str}")
            
            # í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            content = '\n'.join(content_parts)
            
            # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if len(content) < 50:
                continue
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "source": "rdf",
                "type": "maternal_health",
                "uri": subject
            }
            
            # ì¤‘ìš” ì†ì„± ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for k, v in important_attrs.items():
                metadata[f"important_{k}"] = v
            
            # ê³ ìœ  ID ìƒì„±
            doc_id = f"maternal_{idx}_{hash(subject)}"
            
            texts.append(content)
            metadatas.append(metadata)
            ids.append(doc_id)
        
        return texts, metadatas, ids

    @staticmethod
    def embed_maternal_health_data(rdf_file_path: str = None, format: str = None, db_dir: str = None) -> Dict[str, Any]:
        """
        ì‚°ëª¨ ê±´ê°• ê´€ë ¨ RDF ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ì—¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        ê°œì„ ëœ ë²„ì „ - ë” ë§ì€ ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì¶”ì¶œ
        """
        try:
            if rdf_file_path is None:
                rdf_file_path = RAGProcessor.RDF_FILE_PATH
            
            if db_dir is None:
                db_dir = os.path.join(RAGProcessor.DB_DIR, "maternal_health_db")
                
            # RDF ë°ì´í„° ì²˜ë¦¬
            print(f"ğŸ” ì‚°ëª¨ ê±´ê°• RDF íŒŒì¼ ì²˜ë¦¬ ì¤‘: {rdf_file_path}")
            
            # RDF í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
            processor = RDFProcessor(rdf_file_path, format)
            
            # ê°œì„ ëœ ì¿¼ë¦¬ ì‚¬ìš©
            improved_pregnancy_query = """
            PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
            PREFIX owl: <http://www.w3.org/2002/07/owl#>
            PREFIX wsc: <http://www.wellness.ai/schema/class/>

            SELECT DISTINCT ?resource ?label WHERE {
                {
                    # í‚¤ì›Œë“œ ê²€ìƒ‰ (ì›ë˜ íš¨ê³¼ì ì¸ ë°©ì‹)
                    ?resource ?p ?o .
                    FILTER(isURI(?resource))
                    OPTIONAL { ?resource rdfs:label ?label }
                    FILTER(
                        CONTAINS(LCASE(STR(?resource)), "ì„ì‹ ") || 
                        CONTAINS(LCASE(STR(?resource)), "ì¶œì‚°") || 
                        CONTAINS(LCASE(STR(?resource)), "ì‚°ëª¨") || 
                        CONTAINS(LCASE(STR(?resource)), "íƒœì•„") || 
                        CONTAINS(LCASE(STR(?resource)), "ì˜ì•„") || 
                        CONTAINS(LCASE(STR(?resource)), "ì•„ê¸°") || 
                        CONTAINS(LCASE(STR(?resource)), "pregnancy") || 
                        CONTAINS(LCASE(STR(?resource)), "childbirth") || 
                        CONTAINS(LCASE(STR(?resource)), "maternal") || 
                        CONTAINS(LCASE(STR(?resource)), "fetus") || 
                        CONTAINS(LCASE(STR(?resource)), "infant") || 
                        CONTAINS(LCASE(STR(?resource)), "baby") ||
                        CONTAINS(LCASE(STR(?resource)), "pregnancyperiod") || 
                        CONTAINS(LCASE(STR(?resource)), "firsttrimester") || 
                        CONTAINS(LCASE(STR(?resource)), "secondtrimester") || 
                        CONTAINS(LCASE(STR(?resource)), "thirdtrimester")
                    )
                } UNION {
                    # ê°ì²´ê°’ í‚¤ì›Œë“œ ê²€ìƒ‰ (ì›ë˜ íš¨ê³¼ì ì¸ ë°©ì‹)
                    ?resource ?p ?o .
                    FILTER(isURI(?resource))
                    OPTIONAL { ?resource rdfs:label ?label }
                    FILTER(
                        CONTAINS(LCASE(STR(?o)), "ì„ì‹ ") || 
                        CONTAINS(LCASE(STR(?o)), "ì¶œì‚°") || 
                        CONTAINS(LCASE(STR(?o)), "ì‚°ëª¨") || 
                        CONTAINS(LCASE(STR(?o)), "íƒœì•„") || 
                        CONTAINS(LCASE(STR(?o)), "ì˜ì•„") || 
                        CONTAINS(LCASE(STR(?o)), "ì•„ê¸°") || 
                        CONTAINS(LCASE(STR(?o)), "pregnancy") || 
                        CONTAINS(LCASE(STR(?o)), "childbirth") || 
                        CONTAINS(LCASE(STR(?o)), "maternal") || 
                        CONTAINS(LCASE(STR(?o)), "fetus") || 
                        CONTAINS(LCASE(STR(?o)), "infant") || 
                        CONTAINS(LCASE(STR(?o)), "baby") ||
                        CONTAINS(LCASE(STR(?o)), "pregnancyperiod") || 
                        CONTAINS(LCASE(STR(?o)), "firsttrimester") || 
                        CONTAINS(LCASE(STR(?o)), "secondtrimester") || 
                        CONTAINS(LCASE(STR(?o)), "thirdtrimester")
                    )
                }
            }
            LIMIT 2000
            """
            
            # ì„ì‹  ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì¡°íšŒ
            pregnancy_resources = processor.execute_query(improved_pregnancy_query)
            print(f"ğŸ”¢ ì„ì‹  ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ë°œê²¬: {len(pregnancy_resources)}ê°œ")
            
            # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ì¤€ë¹„
            texts = []
            metadatas = []
            ids = []
            
            # ì¤‘ìš” ì†ì„± ëª©ë¡ ì‚¬ìš©
            important_props = RAGProcessor.IMPORTANT_PROPERTIES
            
            # rdflib ê²°ê³¼ í‚¤ í™•ì¸
            if pregnancy_resources and len(pregnancy_resources) > 0:
                first_result = pregnancy_resources[0]
                result_keys = list(first_result.keys())
                print(f"SPARQL ê²°ê³¼ í‚¤: {result_keys}")
                
                # ë¦¬ì†ŒìŠ¤ URIë¥¼ í¬í•¨í•˜ëŠ” í‚¤ ì°¾ê¸°
                resource_key = None
                label_key = None
                
                for key in result_keys:
                    key_str = str(key).lower()
                    if 'resource' in key_str:
                        resource_key = key
                    elif 'label' in key_str:
                        label_key = key
                
                if not resource_key:
                    print("âš ï¸ ë¦¬ì†ŒìŠ¤ URIë¥¼ í¬í•¨í•˜ëŠ” í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return {
                        "file_path": rdf_file_path,
                        "format": format,
                        "resource_count": len(pregnancy_resources),
                        "embedded_resources": 0,
                        "db_dir": db_dir,
                        "warning": "ë¦¬ì†ŒìŠ¤ URIë¥¼ í¬í•¨í•˜ëŠ” í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    }
            
            # ê° ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            processed_uris = set()  # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
            
            for idx, resource in enumerate(pregnancy_resources):
                # rdflib Variable ê°ì²´ë¡œë¶€í„° ê°’ ì¶”ì¶œ
                resource_uri = str(resource[resource_key]) if resource_key and resource_key in resource else None
                
                if not resource_uri or resource_uri in processed_uris:
                    continue
                    
                processed_uris.add(resource_uri)
                    
                # ë¦¬ì†ŒìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                resource_info = processor.get_resource_info(resource_uri)
                
                # ë¦¬ì†ŒìŠ¤ ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸°
                label = str(resource[label_key]) if label_key and label_key in resource and resource[label_key] is not None else None
                if label == "None":  # "None" ë¬¸ìì—´ ì²˜ë¦¬
                    label = None
                
                if not label:
                    label = processor.get_label(resource_uri)
                
                # í…ìŠ¤íŠ¸ ìƒì„±
                text_parts = [f"ì£¼ì œ: {label or resource_uri}"]
                
                # ì¤‘ìš” ì†ì„± ì¶”ì¶œ
                important_attrs = {}
                
                # ì¤‘ìš” ì†ì„± ë¨¼ì € ì¶”ê°€
                for prop_key in resource_info:
                    if any(important_prop in prop_key.lower() for important_prop in important_props):
                        values = resource_info[prop_key]
                        values_str = ", ".join(str(v) for v in values)
                        text_parts.append(f"{prop_key}: {values_str}")
                        
                        # ì¤‘ìš” ì†ì„± ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                        attr_name = prop_key.split('/')[-1].split('#')[-1]
                        important_attrs[attr_name] = values_str
                
                # ë‚˜ë¨¸ì§€ ì†ì„± ì¶”ê°€
                for prop_key in resource_info:
                    if not any(important_prop in prop_key.lower() for important_prop in important_props):
                        values = resource_info[prop_key]
                        values_str = ", ".join(str(v) for v in values)
                        text_parts.append(f"{prop_key}: {values_str}")
                
                # ìµœì¢… í…ìŠ¤íŠ¸ ìƒì„±
                text = "\n".join(text_parts)
                
                # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if len(text) < 50:
                    continue
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = {
                    "source": rdf_file_path,
                    "uri": resource_uri,
                    "label": label or "",
                }
                
                # ì¤‘ìš” ì†ì„± ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for k, v in important_attrs.items():
                    metadata[f"important_{k}"] = v
                
                # ë°ì´í„° ì¶”ê°€
                texts.append(text)
                metadatas.append(metadata)
                ids.append(f"maternal_{idx}")
            
            # ì„ë² ë”© ìƒì„±
            print(f"ğŸ§  ë°ì´í„° ì„ë² ë”© ìƒì„± ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
            
            # í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if len(texts) == 0:
                print("âš ï¸ ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return {
                    "file_path": rdf_file_path,
                    "format": format,
                    "resource_count": len(pregnancy_resources),
                    "embedded_resources": 0,
                    "db_dir": db_dir,
                    "warning": "ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # ë²¡í„° ì„ë² ë”© ìƒì„±
            embeddings = RAGProcessor.create_embeddings(texts)
            
            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            if not os.path.exists(db_dir):
                os.makedirs(db_dir, exist_ok=True)
                
            embedding_function = OpenAIEmbeddings(model="text-embedding-3-small")
            vectorstore = Chroma(
                persist_directory=db_dir,
                embedding_function=embedding_function,
                collection_name="maternal_health_knowledge"
            )
            
            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            print(f"ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹œì‘")
            
            vectorstore._collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            print(f"âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
            
            return {
                "file_path": rdf_file_path,
                "format": format,
                "resource_count": len(pregnancy_resources),
                "embedded_resources": len(texts),
                "db_dir": db_dir
            }
            
        except Exception as e:
            print(f"âŒ RDF ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            traceback.print_exc()
            return {
                "error": str(e),
                "file_path": rdf_file_path,
                "format": format
            }

    @staticmethod
    def embed_maternal_health_data_from_ntriples(rdf_file_path: str = None, format: str = None, db_dir: str = None) -> Dict[str, Any]:
        """
        n-triples íŒŒì¼ì—ì„œ ì§ì ‘ ì‚°ëª¨ ê±´ê°• ë°ì´í„°ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤.
        """
        try:
            print(f"ì„ë² ë”© ì‹œì‘: {rdf_file_path}")
            
            if rdf_file_path is None:
                rdf_file_path = RAGProcessor.RDF_FILE_PATH
                
            if db_dir is None:
                db_dir = os.path.join(RAGProcessor.DB_DIR, "maternal_health_db")
                
            # n-triples íŒŒì¼ íŒŒì‹±
            print(f"n-triples íŒŒì¼ íŒŒì‹± ì¤‘: {rdf_file_path}")
            subjects = RAGProcessor.parse_n_triples(rdf_file_path)
            
            # ì„ì‹  ê´€ë ¨ í‚¤ì›Œë“œ ëª©ë¡
            pregnancy_keywords = [
                "ì„ì‹ ", "ì¶œì‚°", "ì‚°ëª¨", "íƒœì•„", "ì˜ì•„", "ì•„ê¸°", "pregnancy", 
                "childbirth", "maternal", "fetus", "infant", "baby",
                "PregnancyPeriod", "FirstTrimester", "SecondTrimester", "ThirdTrimester"
            ]
            
            # ì„ì‹  ê´€ë ¨ ì£¼ì œ í•„í„°ë§
            pregnancy_subjects = {}
            for subject, triples in subjects.items():
                # ì£¼ì œ URIì— ì„ì‹  ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if any(keyword in subject for keyword in pregnancy_keywords):
                    pregnancy_subjects[subject] = triples
                    continue
                    
                # íŠ¸ë¦¬í”Œì— ì„ì‹  ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                for _, obj in triples:
                    if any(keyword in obj for keyword in pregnancy_keywords):
                        pregnancy_subjects[subject] = triples
                        break
            
            print(f"ì„ì‹  ê´€ë ¨ ì£¼ì œ {len(pregnancy_subjects)}ê°œ ë°œê²¬")
            
            # ì„ë² ë”© ìƒì„±
            texts, metadatas, ids = RAGProcessor.create_embeddings_from_subjects(pregnancy_subjects)
            
            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            if not os.path.exists(db_dir):
                os.makedirs(db_dir, exist_ok=True)
                
            embedding_function = OpenAIEmbeddings(model="text-embedding-3-small")
            vectorstore = Chroma(
                persist_directory=db_dir,
                embedding_function=embedding_function,
                collection_name="maternal_health_knowledge"
            )
            
            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            print(f"ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹œì‘")
            
            vectorstore._collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            print(f"âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
            
            return {
                "file_path": rdf_file_path,
                "format": format,
                "triple_count": len(subjects),
                "embedded_resources": len(texts),
                "db_dir": db_dir
            }
            
        except Exception as e:
            print(f"âŒ n-triples íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            traceback.print_exc()
            return {
                "error": str(e),
                "file_path": rdf_file_path,
                "format": format
            }

    @staticmethod
    def load_and_preprocess_csv(csv_pattern):
        """CSV íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ë°˜í™˜."""
        csv_files = glob.glob(csv_pattern)
        if not csv_files:
            print(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_pattern}")
            return None
        print(f"ì²˜ë¦¬í•  CSV íŒŒì¼: {len(csv_files)}ê°œ")
        for file in csv_files:
            print(f"- {file}")
        return csv_files

    @staticmethod
    def filter_processed_files(csv_files):
        """ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì„ ì œì™¸í•˜ê³  ìƒˆë¡œìš´ íŒŒì¼ ëª©ë¡ë§Œ ë°˜í™˜."""
        processed_files = set(RAG_DB.objects.values_list('file_path', flat=True))
        new_files = [f for f in csv_files if f not in processed_files]
        print(f"ì²˜ë¦¬í•  ìƒˆë¡œìš´ CSV íŒŒì¼: {len(new_files)}ê°œ")
        return new_files

    @staticmethod
    def initialize_chroma_db():
        """Chroma DBë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ê¸°ì¡´ DBë¥¼ ë¡œë“œ."""
        db_dir = RAGProcessor.DB_DIR
        print("\n=== Chroma DB ìƒíƒœ ===")
        print(f"ì‚¬ìš© ì¤‘ì¸ DB ê²½ë¡œ: {db_dir}")
        print(f"ì ˆëŒ€ ê²½ë¡œ: {os.path.abspath(db_dir)}")
        
        # DB íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if os.path.exists(f"{db_dir}/chroma.sqlite3"):
            print(f"chroma.sqlite3 íŒŒì¼ í¬ê¸°: {os.path.getsize(f'{db_dir}/chroma.sqlite3')} bytes")
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", chunk_size=1000)

        if os.path.exists(db_dir) and os.path.exists(f"{db_dir}/chroma.sqlite3"):
            print("ê¸°ì¡´ Chroma DB ë¡œë“œ ì¤‘...")
            vectorstore = Chroma(
                persist_directory=db_dir,
                embedding_function=embeddings,
                collection_name="korean_dialogue"
            )
            existing_ids = set(vectorstore._collection.get()['ids'])
            print(f"ê¸°ì¡´ ë¬¸ì„œ ìˆ˜: {len(existing_ids)}")
        else:
            print("ìƒˆë¡œìš´ Chroma DB ìƒì„±")
            vectorstore = None
            existing_ids = set()

        return vectorstore, existing_ids

    @staticmethod
    def load_csv_with_metadata(csv_file):
        """CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            print(f"\níŒŒì¼ ì²˜ë¦¬ ì¤‘: {csv_file}")
            
            # ì¤‘ìš” ì†ì„± ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            important_props = RAGProcessor.IMPORTANT_PROPERTIES
            
            # pandasê°€ ì—†ìœ¼ë©´ CSVLoader ì‚¬ìš©
            if not PANDAS_AVAILABLE:
                loader = CSVLoader(file_path=csv_file)
                docs = loader.load()
                
                # ê° ë¬¸ì„œì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for i, doc in enumerate(docs):
                    # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë³´ì¡´
                    doc.metadata["source"] = csv_file
                    doc.metadata["row_index"] = i
                    
                    # ì¤‘ìš” ì†ì„± ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                    content = doc.page_content
                    for prop in important_props:
                        if prop in content.lower():
                            # í•´ë‹¹ ì†ì„±ì´ í¬í•¨ëœ ì¤„ ì°¾ê¸°
                            lines = content.split('\n')
                            for line in lines:
                                if prop in line.lower():
                                    # ì†ì„± ì´ë¦„ê³¼ ê°’ ì¶”ì¶œ
                                    if ':' in line:
                                        attr_name = line.split(':', 1)[0].strip()
                                        attr_value = line.split(':', 1)[1].strip()
                                        doc.metadata[f"important_{attr_name}"] = attr_value
                
                return docs
                
            # pandasë¥¼ ì‚¬ìš©í•œ ì²˜ë¦¬
            df = pd.read_csv(csv_file)
            docs = []
            
            # ê° í–‰ì„ ë¬¸ì„œë¡œ ë³€í™˜
            for i, row in df.iterrows():
                # í…ìŠ¤íŠ¸ ìƒì„±
                text_parts = []
                important_attrs = {}
                
                # ì¤‘ìš” ì†ì„± ë¨¼ì € ì¶”ê°€
                for col in df.columns:
                    if any(important_prop in col.lower() for important_prop in important_props) and pd.notna(row[col]):
                        text_parts.append(f"{col}: {row[col]}")
                        important_attrs[col] = str(row[col])
                
                # ë‚˜ë¨¸ì§€ ì†ì„± ì¶”ê°€
                for col in df.columns:
                    if not any(important_prop in col.lower() for important_prop in important_props) and pd.notna(row[col]):
                        text_parts.append(f"{col}: {row[col]}")
                        
                        # ê°’ì— ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                        value = str(row[col])
                        for prop in important_props:
                            if prop in value.lower():
                                important_attrs[f"contains_{prop}_in_{col}"] = "true"
                
                # ìµœì¢… í…ìŠ¤íŠ¸ ìƒì„±
                text = "\n".join(text_parts)
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = {
                    "source": csv_file,
                    "row_index": i,
                }
                
                # ë©”íƒ€ë°ì´í„°ì— ì—´ ì •ë³´ ì¶”ê°€
                for col in df.columns:
                    if pd.notna(row[col]):
                        metadata[col] = str(row[col])
                
                # ì¤‘ìš” ì†ì„± ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for k, v in important_attrs.items():
                    metadata[f"important_{k}"] = v
                
                # ë¬¸ì„œ ìƒì„±
                doc = Document(
                    page_content=text,
                    metadata=metadata
                )
                
                docs.append(doc)
            
            print(f"CSV íŒŒì¼ì—ì„œ {len(docs)}ê°œ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
            return docs
            
        except Exception as e:
            print(f"âŒ CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            traceback.print_exc()
            return []

    @staticmethod
    def filter_new_documents(docs, existing_ids, csv_file):
        """ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¬¸ì„œë¥¼ ì œì™¸í•˜ê³  ìƒˆ ë¬¸ì„œë§Œ í•„í„°ë§."""
        new_docs = []
        for idx, doc in enumerate(docs):
            doc.metadata['source_file'] = os.path.basename(csv_file)
            unique_id = str(uuid.uuid4())
            doc_id = f"doc_{unique_id}_{idx}"
            if doc_id not in existing_ids:
                doc.metadata['doc_id'] = doc_id
                new_docs.append(doc)
        print(f"ìƒˆë¡œìš´ ë¬¸ì„œ ë°œê²¬: {len(new_docs)}ê°œ")
        return new_docs

    @staticmethod
    def split_documents(docs):
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• ."""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        splits = text_splitter.split_documents(docs)
        print(f"ë¶„í•  ì™„ë£Œ: {len(splits)}ê°œ ì²­í¬")
        return splits

    @staticmethod
    def prepare_data_for_chroma(splits):
        """Chroma DBì— ì €ì¥í•  í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°, ID ì¤€ë¹„."""
        texts, metadatas, ids = [], [], []
        
        # ì¤‘ìš” ì†ì„± ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        important_props = RAGProcessor.IMPORTANT_PROPERTIES
        
        for doc in splits:
            # í…ìŠ¤íŠ¸ ë‚´ìš©ì— ì¤‘ìš” ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
            content = doc.page_content
            important_attrs = {}
            
            # ì¤‘ìš” ì†ì„±ì„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
            for prop in important_props:
                if prop in content.lower():
                    # í•´ë‹¹ ì†ì„±ì´ í¬í•¨ëœ ì¤„ ì°¾ê¸°
                    lines = content.split('\n')
                    for line in lines:
                        if prop in line.lower():
                            # ì†ì„± ì´ë¦„ê³¼ ê°’ ì¶”ì¶œ
                            if ':' in line:
                                attr_name = line.split(':', 1)[0].strip()
                                attr_value = line.split(':', 1)[1].strip()
                                important_attrs[attr_name] = attr_value
            
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ê°€
            texts.append(f"content: {content}")
            
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
            metadata = {
                "emotion": doc.metadata.get('emotion', ''),
                "source": doc.metadata.get('source', ''),
                "source_file": doc.metadata.get('source_file', '')
            }
            
            # ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³µì‚¬
            for k, v in doc.metadata.items():
                if k not in metadata:
                    metadata[k] = v
                    
            # ì¤‘ìš” ì†ì„± ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for k, v in important_attrs.items():
                metadata[f"important_{k}"] = v
                
            metadatas.append(metadata)
            ids.append(doc.metadata.get('doc_id'))
        
        return texts, metadatas, ids

    @staticmethod
    async def create_embeddings_async(texts: List[str], pbar: tqdm, batch_size: int = 20, concurrent_tasks: int = 5) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë¹„ë™ê¸°ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        embedding_function = OpenAIEmbeddings(
            model="text-embedding-3-small",
            chunk_size=1000
        )
        
        all_embeddings = []
        batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]
        semaphore = asyncio.Semaphore(concurrent_tasks)
        async def process_batch(batch):
            async with semaphore:
                await asyncio.sleep(0.1)
                return await embedding_function.aembed_documents(batch)
        tasks = [process_batch(batch) for batch in batches]
        for coro in asyncio.as_completed(tasks):
            embeddings = await coro
            if embeddings:
                all_embeddings.extend(embeddings)
                pbar.update(batch_size)
        return all_embeddings

    @staticmethod
    def get_optimal_embedding_params(num_texts: int) -> tuple[int, int]:
        """
        ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì´ ê°œìˆ˜(num_texts)ì™€ CPU ì½”ì–´ ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì ì˜ batch_sizeì™€
        concurrent_tasks ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        Returns:
            tuple: (batch_size, concurrent_tasks)
        """
        import multiprocessing
        cpu_count = multiprocessing.cpu_count()
        # í…ìŠ¤íŠ¸ ìˆ˜ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” ê°„ë‹¨í•œ heuristic
        if num_texts < 100:
            batch_size = 10
        elif num_texts < 1000:
            batch_size = 20
        else:
            batch_size = 50

        # ë™ì‹œ ì‹¤í–‰ íƒœìŠ¤í¬ëŠ” CPU ì½”ì–´ ìˆ˜ì™€ ìƒí•œê°’ 10ì„ ê³ ë ¤
        concurrent_tasks = min(10, cpu_count)
        print(
            f"Optimal Parameters determined: batch_size={batch_size}, "
            f"concurrent_tasks={concurrent_tasks} based on {num_texts} texts and {cpu_count} CPUs"
        )
        return batch_size, concurrent_tasks

    @staticmethod
    def create_embeddings(texts: List[str]) -> List[List[float]]:
        """ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë¹„ë™ê¸° ì„ë² ë”© ìƒì„±ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ìˆ˜ì— ë”°ë¼ ìµœì ì˜ batch_sizeì™€ concurrent_tasksë¥¼ ê³„ì‚°í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        batch_size, concurrent_tasks = RAGProcessor.get_optimal_embedding_params(len(texts))
        with tqdm(total=len(texts), desc="ì„ë² ë”© ìƒì„± ì¤‘") as pbar:
            embeddings = asyncio.run(
                RAGProcessor.create_embeddings_async(texts, pbar, batch_size, concurrent_tasks)
            )
        return embeddings

    @staticmethod
    def visualize_embedding_progress(total_files, processed_files, current_file, progress_pct):
        """ì„ë² ë”© ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""
        terminal_width = os.get_terminal_size().columns - 10
        bar_width = terminal_width - 40
        
        # ì „ì²´ ì§„í–‰ ìƒí™© ê³„ì‚°
        overall_progress = (processed_files / total_files) * 100 if total_files > 0 else 0
        
        # ì „ì²´ ì§„í–‰ ìƒí™© ë°”
        overall_filled = int(bar_width * overall_progress / 100)
        overall_bar = f"[{'=' * overall_filled}{' ' * (bar_width - overall_filled)}]"
        
        # í˜„ì¬ íŒŒì¼ ì§„í–‰ ìƒí™© ë°”
        file_filled = int(bar_width * progress_pct / 100)
        file_bar = f"[{'=' * file_filled}{' ' * (bar_width - file_filled)}]"
        
        # ì¶œë ¥
        print("\033[H\033[J")  # í™”ë©´ ì§€ìš°ê¸°
        print(f"ğŸ“Š ì„ë² ë”© ì§„í–‰ ìƒí™©")
        print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"ğŸ“ ì „ì²´ íŒŒì¼: {total_files}ê°œ ì¤‘ {processed_files}ê°œ ì™„ë£Œ ({overall_progress:.1f}%)")
        print(f"{overall_bar}")
        print(f"ğŸ“„ í˜„ì¬ íŒŒì¼: {os.path.basename(current_file)} ({progress_pct:.1f}%)")
        print(f"{file_bar}")
        print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    @staticmethod
    def process_files(csv_files: List[str], existing_ids: set, vectorstore, db_dir: str):
        """CSV íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ê³  ì§„í–‰ìƒí™©ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""
        total_new_docs = 0
        processed_count = 0
        total_files = len(csv_files)

        print("\n=== CSV íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ===")
        for idx, csv_file in enumerate(csv_files):
            try:
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                RAGProcessor.visualize_embedding_progress(
                    total_files, 
                    processed_count, 
                    csv_file, 
                    (idx / total_files) * 100 if total_files > 0 else 0
                )
                
                # CSV íŒŒì¼ ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
                docs = RAGProcessor.load_csv_with_metadata(csv_file)
                if not docs:
                    continue

                # ìƒˆ ë¬¸ì„œ í•„í„°ë§
                new_docs = RAGProcessor.filter_new_documents(docs, existing_ids, csv_file)
                if not new_docs:
                    continue

                # ë¬¸ì„œ ë¶„í• 
                total_new_docs += len(new_docs)
                splits = RAGProcessor.split_documents(new_docs)
                
                # ë°ì´í„° ì¤€ë¹„
                texts, metadatas, ids = RAGProcessor.prepare_data_for_chroma(splits)
                
                print(f"\nğŸ“„ [{os.path.basename(csv_file)}] ì²˜ë¦¬ ì¤‘...")
                print(f"   - í…ìŠ¤íŠ¸ ìˆ˜: {len(texts)}ê°œ")
                
                # ì„ì‹œ ì €ì¥ëœ ì„ë² ë”© í™•ì¸
                temp_embeddings = RAGProcessor.load_temp_embeddings(csv_file)
                
                if temp_embeddings is not None:
                    print("ğŸ’¾ ê¸°ì¡´ ì„ì‹œ ì„ë² ë”© ì‚¬ìš©")
                    embeddings = temp_embeddings
                else:
                    print("ğŸ”„ ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± ì‹œì‘")
                    embeddings = RAGProcessor.create_embeddings(texts)
                    RAGProcessor.save_temp_embeddings(csv_file, embeddings)
                
                # Chroma DB ì—…ë°ì´íŠ¸
                vectorstore = RAGProcessor.update_chroma_db(
                    vectorstore, texts, embeddings, metadatas, ids, db_dir
                )
                
                # ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
                RAGProcessor.save_processed_file_info(csv_file)
                processed_count += 1
                
                # ìµœì¢… ì§„í–‰ ìƒí™© í‘œì‹œ
                RAGProcessor.visualize_embedding_progress(
                    total_files, 
                    processed_count, 
                    csv_file, 
                    100.0
                )
                
                print(f"âœ… [{os.path.basename(csv_file)}] ì²˜ë¦¬ ì™„ë£Œ\n")

            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({os.path.basename(csv_file)}): {e}")
                continue

        return vectorstore, total_new_docs, processed_count

    @staticmethod
    def update_chroma_db(vectorstore, texts, embeddings, metadatas, ids, db_dir):
        """Chroma DBì— ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¶”ê°€í•˜ê³  ì§„í–‰ìƒí™©ì„ í‘œì‹œí•©ë‹ˆë‹¤."""
        MAX_BATCH_SIZE = 5000

        if vectorstore is None:
            print("ğŸ”¨ ìƒˆë¡œìš´ Chroma DB ìƒì„± ì¤‘...")
            embedding_function = OpenAIEmbeddings(model="text-embedding-3-small")
            vectorstore = Chroma(
                persist_directory=db_dir,
                embedding_function=embedding_function,
                collection_name="korean_dialogue"
            )

        total_batches = (len(texts) + MAX_BATCH_SIZE - 1) // MAX_BATCH_SIZE
        print(f"ğŸ“¦ Chroma DB ì—…ë°ì´íŠ¸ ì‹œì‘ (ì´ {total_batches}ê°œ ë°°ì¹˜)")
        
        for i in tqdm(range(0, len(texts), MAX_BATCH_SIZE), desc="ğŸ’« DB ì—…ë°ì´íŠ¸"):
            end_idx = min(i + MAX_BATCH_SIZE, len(texts))
            vectorstore._collection.add(
                embeddings=embeddings[i:end_idx],
                documents=texts[i:end_idx],
                metadatas=metadatas[i:end_idx],
                ids=ids[i:end_idx]
            )
        
        print(f"âœ¨ DB ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ {len(texts)}ê°œ ë¬¸ì„œ)")
        return vectorstore

    @staticmethod
    async def async_update_chroma_db(vectorstore, texts, embeddings, metadatas, ids, db_dir):
        """
        ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ Chroma DBë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        ìµœëŒ€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ 5000ê°œë¥¼ ê³ ë ¤í•˜ì—¬ vectorstore._collection.add í˜¸ì¶œì„
        asyncio.to_threadë¡œ ê°ì‹¸ì„œ ë™ì‹œì— ì‹¤í–‰í•©ë‹ˆë‹¤.

        Returns:
            ì—…ë°ì´íŠ¸ëœ vectorstore ì¸ìŠ¤í„´ìŠ¤.
        """
        MAX_BATCH_SIZE = 5000
        total_batches = (len(texts) + MAX_BATCH_SIZE - 1) // MAX_BATCH_SIZE
        print(f"ğŸ“¦ ë¹„ë™ê¸° Chroma DB ì—…ë°ì´íŠ¸ ì‹œì‘ (ì´ {total_batches}ê°œ ë°°ì¹˜)")
        tasks = []
        for i in range(0, len(texts), MAX_BATCH_SIZE):
            end_idx = min(i + MAX_BATCH_SIZE, len(texts))
            tasks.append(
                asyncio.to_thread(
                    vectorstore._collection.add,
                    embeddings=embeddings[i:end_idx],
                    documents=texts[i:end_idx],
                    metadatas=metadatas[i:end_idx],
                    ids=ids[i:end_idx]
                )
            )
        await asyncio.gather(*tasks)
        print(f"âœ¨ ë¹„ë™ê¸° DB ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ {len(texts)}ê°œ ë¬¸ì„œ)")
        return vectorstore

    @staticmethod
    def save_processed_file_info(csv_file):
        """ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´ë¥¼ DBì— ì €ì¥."""
        RAG_DB.objects.create(file_name=os.path.basename(csv_file), file_path=csv_file)

    @staticmethod
    def get_temp_embedding_path(file_name):
        """ì„ì‹œ ì„ë² ë”© íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not os.path.exists(RAGProcessor.TEMP_DIR):
            os.makedirs(RAGProcessor.TEMP_DIR)
        return os.path.join(RAGProcessor.TEMP_DIR, f"{os.path.basename(file_name)}.pkl")

    @staticmethod
    def save_temp_embeddings(file_name, data):
        """ì„ë² ë”© ë°ì´í„°ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        temp_path = RAGProcessor.get_temp_embedding_path(file_name)
        with open(temp_path, 'wb') as f:
            pickle.dump(data, f)

    @staticmethod
    def load_temp_embeddings(file_name):
        """ì„ì‹œ ì €ì¥ëœ ì„ë² ë”© ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        temp_path = RAGProcessor.get_temp_embedding_path(file_name)
        if os.path.exists(temp_path):
            with open(temp_path, 'rb') as f:
                return pickle.load(f)
        return None

    @staticmethod
    def process_conversation_json(conversation, existing_ids, vectorstore):
        """JSON ëŒ€í™” ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ì„ë² ë”©í•©ë‹ˆë‹¤."""
        try:
            # ëŒ€í™” ì •ë³´ ì¶”ì¶œ
            info = conversation.get("info", {})
            utterances = conversation.get("utterances", [])
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            base_metadata = {
                "source": "conversation",
                "conversation_id": info.get("id", ""),
                "domain": info.get("domain", ""),
                "topic": info.get("topic", ""),
                "scenario": info.get("scenario", "")
            }
            
            # ì¤‘ìš” ì†ì„± ëª©ë¡ ì‚¬ìš©
            important_props = RAGProcessor.IMPORTANT_PROPERTIES
            
            # í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            texts = []
            metadatas = []
            ids = []
            
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ì 
            context = []
            
            # ê° ë°œí™” ì²˜ë¦¬
            for idx, utterance in enumerate(utterances):
                # ë°œí™” ì •ë³´ ì¶”ì¶œ
                speaker = utterance.get("speaker", "")
                text = utterance.get("text", "")
                
                # ë¹ˆ ë°œí™” ê±´ë„ˆë›°ê¸°
                if not text.strip():
                    continue
                
                # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                context.append(f"{speaker}: {text}")
                
                # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ìœ ì§€ (ìµœëŒ€ 5ê°œ ë°œí™”)
                if len(context) > 5:
                    context = context[-5:]
                
                # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ìƒì„±
                context_str = "\n".join(context)
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = base_metadata.copy()
                metadata.update({
                    "speaker": speaker,
                    "utterance_index": idx,
                    "text": text
                })
                
                # ì¤‘ìš” ì†ì„± ì‹ë³„ ë° ì¶”ê°€
                important_attrs = {}
                
                # í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš” ì†ì„± ì°¾ê¸°
                for prop in important_props:
                    if prop in text.lower():
                        important_attrs[f"contains_{prop}"] = "true"
                
                # ê° ë°œí™”ì˜ ëª¨ë“  í•„ë“œì—ì„œ ì¤‘ìš” ì†ì„± ì°¾ê¸°
                for key, value in utterance.items():
                    if isinstance(value, str):
                        # ê°’ì´ ì¤‘ìš” ì†ì„±ì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
                        for prop in important_props:
                            if prop in value.lower():
                                important_attrs[f"contains_{prop}_in_{key}"] = "true"
                                
                        # í‚¤ê°€ ì¤‘ìš” ì†ì„±ê³¼ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        if any(prop in key.lower() for prop in important_props):
                            important_attrs[key] = value
                
                # ì¤‘ìš” ì†ì„± ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for k, v in important_attrs.items():
                    metadata[k] = v
                
                # ê³ ìœ  ID ìƒì„±
                doc_id = f"conv_{info.get('id', '')}_{idx}"
                
                # ì´ë¯¸ ì²˜ë¦¬ëœ ID ê±´ë„ˆë›°ê¸°
                if doc_id in existing_ids:
                    continue
                
                # ë°ì´í„° ì¶”ê°€
                texts.append(context_str)
                metadatas.append(metadata)
                ids.append(doc_id)
            
            # ì„ë² ë”© ìƒì„± ë° ì €ì¥
            if texts:
                print(f"ğŸ§  ëŒ€í™” ì„ë² ë”© ìƒì„± ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
                embeddings = RAGProcessor.create_embeddings(texts)
                
                print(f"ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹œì‘")
                RAGProcessor.update_chroma_db(
                    vectorstore, texts, embeddings, metadatas, ids, RAGProcessor.DB_DIR
                )
                
                print(f"âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
            
            return vectorstore, len(texts), len(texts)
            
        except Exception as e:
            print(f"âŒ ëŒ€í™” ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return vectorstore, 0, 0

    @staticmethod
    def process_rdf_data(rdf_file_path: str = None, format: str = None) -> Dict[str, Any]:
        """
        RDF ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ì„ë² ë”©í•©ë‹ˆë‹¤.
        """
        try:
            # RDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
            if rdf_file_path is None:
                rdf_file_path = RAGProcessor.RDF_FILE_PATH
            
            # RDF í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
            processor = RDFProcessor(rdf_file_path, format)
            
            # ì„ì‹  ê´€ë ¨ í‚¤ì›Œë“œ ëª©ë¡
            pregnancy_keywords = [
                "ì„ì‹ ", "ì¶œì‚°", "ì‚°ëª¨", "íƒœì•„", "ì˜ì•„", "ì•„ê¸°", "pregnancy", 
                "childbirth", "maternal", "fetus", "infant", "baby",
                "PregnancyPeriod", "FirstTrimester", "SecondTrimester", "ThirdTrimester"
            ]
            
            # ì„ì‹  ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì¡°íšŒ ì¿¼ë¦¬
            pregnancy_resources_query = f"""
            PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
            
            SELECT DISTINCT ?resource ?label WHERE {{
                {{
                    ?resource ?p ?o .
                    FILTER(isURI(?resource))
                    OPTIONAL {{ ?resource rdfs:label ?label }}
                    FILTER(
                        {" || ".join([f'CONTAINS(LCASE(STR(?resource)), "{keyword.lower()}")' for keyword in pregnancy_keywords])}
                    )
                }} UNION {{
                    ?resource ?p ?o .
                    FILTER(isURI(?resource))
                    OPTIONAL {{ ?resource rdfs:label ?label }}
                    FILTER(
                        {" || ".join([f'CONTAINS(LCASE(STR(?o)), "{keyword.lower()}")' for keyword in pregnancy_keywords])}
                    )
                }}
            }}
            LIMIT 1000
            """
            
            # ì„ì‹  ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì¡°íšŒ
            resource_results = processor.execute_query(pregnancy_resources_query)
            print(f"ğŸ”¢ ì„ì‹  ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ë°œê²¬: {len(resource_results)}ê°œ")
            
            # ë¦¬ì†ŒìŠ¤ URI ì¶”ì¶œ
            resource_uris = [result["resource"] for result in resource_results]
            
            print(f"ë””ë²„ê¹…: ë°œê²¬ëœ ë¦¬ì†ŒìŠ¤ URI ìƒ˜í”Œ: {resource_uris[:5] if len(resource_uris) > 5 else resource_uris}")
            
            # ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            texts = []
            metadatas = []
            ids = []
            
            # ì¤‘ìš” ì†ì„± ëª©ë¡ ì‚¬ìš©
            important_props = RAGProcessor.IMPORTANT_PROPERTIES
            
            # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
            processed_resources = 0
            total_text_parts = 0
            
            # ê° ë¦¬ì†ŒìŠ¤ì— ëŒ€í•œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            for i, uri in enumerate(tqdm(resource_uris, desc="ë¦¬ì†ŒìŠ¤ ì²˜ë¦¬")):
                # ë¦¬ì†ŒìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                resource_info = processor.get_resource_info(uri)
                
                # ë¦¬ì†ŒìŠ¤ ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸°
                label = next((result.get("label") for result in resource_results if result.get("resource") == uri), None)
                if label is None:
                    label = processor.get_label(uri)
                
                # í…ìŠ¤íŠ¸ ìƒì„±
                text_parts = [f"ì£¼ì œ: {label or uri}"]
                
                # ì¤‘ìš” ì†ì„± ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                important_attrs = {}
                
                # ì¤‘ìš” ì†ì„± ë¨¼ì € ì¶”ê°€
                for prop_key in resource_info:
                    if any(important_prop in prop_key.lower() for important_prop in important_props):
                        values = resource_info[prop_key]
                        values_str = ", ".join(str(v) for v in values)
                        text_parts.append(f"{prop_key}: {values_str}")
                        
                        # ì¤‘ìš” ì†ì„± ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                        attr_name = prop_key.split('/')[-1].split('#')[-1]
                        important_attrs[attr_name] = values_str
                
                # ë‚˜ë¨¸ì§€ ì†ì„± ì¶”ê°€
                for prop_key in resource_info:
                    if not any(important_prop in prop_key.lower() for important_prop in important_props):
                        values = resource_info[prop_key]
                        values_str = ", ".join(str(v) for v in values)
                        text_parts.append(f"{prop_key}: {values_str}")
                        
                        # ê°ì²´ì— ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ì—ë„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                        for value in values:
                            for prop in important_props:
                                if isinstance(value, str) and prop in value.lower():
                                    attr_name = prop_key.split('/')[-1].split('#')[-1]
                                    important_attrs[f"related_{attr_name}"] = str(value)
                
                # ìµœì¢… í…ìŠ¤íŠ¸ ìƒì„±
                text = "\n".join(text_parts)
                
                # í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê±´ë„ˆë›°ê¸°)
                if len(text) < 50:
                    print(f"ê²½ê³ : í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(text)}ì) - {uri}")
                    continue
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = {
                    "source": rdf_file_path,
                    "uri": uri,
                    "label": label or "",
                }
                
                # ì¤‘ìš” ì†ì„± ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for k, v in important_attrs.items():
                    metadata[f"important_{k}"] = v
                
                # ë°ì´í„° ì¶”ê°€
                texts.append(text)
                metadatas.append(metadata)
                ids.append(f"rdf-{i}")
                
                # ë””ë²„ê¹… ì •ë³´ ì—…ë°ì´íŠ¸
                processed_resources += 1
                total_text_parts += len(text_parts)
            
            # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
            print(f"ë””ë²„ê¹…: ì²˜ë¦¬ëœ ë¦¬ì†ŒìŠ¤ ìˆ˜: {processed_resources}")
            print(f"ë””ë²„ê¹…: í‰ê·  í…ìŠ¤íŠ¸ ë¶€ë¶„ ìˆ˜: {total_text_parts / processed_resources if processed_resources > 0 else 0}")
            
            # ì„ë² ë”© ìƒì„±
            print(f"ğŸ§  ë°ì´í„° ì„ë² ë”© ìƒì„± ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
            
            # í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if len(texts) == 0:
                print("âš ï¸ ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return {
                    "file_path": rdf_file_path,
                    "format": format,
                    "resource_count": len(resource_uris),
                    "embedded_resources": 0,
                    "db_dir": RAGProcessor.DB_DIR,
                    "warning": "ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
                }
                
            embeddings = RAGProcessor.create_embeddings(texts)
            
            # Chroma DB ì´ˆê¸°í™”
            vectorstore, _ = RAGProcessor.initialize_chroma_db()
            
            # Chroma DB ì—…ë°ì´íŠ¸
            print(f"ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹œì‘")
            RAGProcessor.update_chroma_db(
                vectorstore, texts, embeddings, metadatas, ids, RAGProcessor.DB_DIR
            )
            
            print(f"âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
            
            return {
                "file_path": rdf_file_path,
                "format": format,
                "resource_count": len(resource_uris),
                "embedded_resources": len(texts),
                "db_dir": RAGProcessor.DB_DIR
            }
            
        except Exception as e:
            print(f"âŒ RDF ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            traceback.print_exc()
            return {
                "error": str(e),
                "file_path": rdf_file_path,
                "format": format
            }

class RAGQuery:
    @staticmethod
    def create_qa_chain():
        """ê³µìœ ëœ DB_DIRì„ ì‚¬ìš©í•˜ì—¬ QA ì²´ì¸ ìƒì„±"""
        db_dir = RAGProcessor.DB_DIR
        vectorstore = Chroma(
            persist_directory=db_dir,
            embedding_function=OpenAIEmbeddings(
                model="text-embedding-3-small"
            ),
            collection_name="korean_dialogue"
        )
        
        # í•„í„° ì œê±°í•˜ê³  í…ŒìŠ¤íŠ¸
        retriever = vectorstore.as_retriever(
            search_kwargs={"k": 10},  # ìƒìœ„ 10ê°œ ê²°ê³¼
            filter={"emotion": "happy"},
        )
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        print(f"ì»¬ë ‰ì…˜ ë‚´ ë¬¸ì„œ ìˆ˜: {vectorstore._collection.count()}")
        
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7
        )

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìˆ˜ì •: ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ ë¦¬íŠ¸ë¦¬ë¸Œëœ ë¬¸ì„œë¥¼ ë³„ë„ì˜ í‚¤ë¡œ ì „ë‹¬
        template = """ë‹¹ì‹ ì€ ì‚°ëª¨ë¥¼ ìœ„í•œ ì¹œì ˆí•œ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ëª¨ë“  ì§ˆë¬¸ì€ ì‚°ëª¨ê°€ í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì‘ë‹µí•´ì£¼ì„¸ìš”.
        
        ê²€ìƒ‰ëœ ì •ë³´:
        {retrieved_context}
        
        ì‚°ëª¨ì˜ ì§ˆë¬¸:
        {question}
        
        ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
        1. ì‚°ëª¨ì˜ ì…ì¥ì—ì„œ ê³µê°í•˜ê³  ì§€ì§€í•˜ëŠ” íƒœë„ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
        2. ì‚°ëª¨ì˜ ë¶ˆì•ˆì„ ì¤„ì´ê³  ì•ˆì‹¬ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        3. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ë”°ëœ»í•œ ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        4. "~ë‹˜"ê³¼ ê°™ì€ ì¡´ì¹­ì„ ì‚¬ìš©í•˜ì—¬ ì¡´ì¤‘í•˜ëŠ” íƒœë„ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”.
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm
        
        return retriever, chain

    @staticmethod
    def create_maternal_health_qa_chain():
        """ì‚°ëª¨ ê±´ê°• ë°ì´í„°ë¥¼ ìœ„í•œ QA ì²´ì¸ ìƒì„±"""
        maternal_db_dir = os.path.join(RAGProcessor.DB_DIR, "maternal_health_db")
        
        # ì‚°ëª¨ ê±´ê°• ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not os.path.exists(maternal_db_dir):
            print(f"ì‚°ëª¨ ê±´ê°• ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {maternal_db_dir}")
            # ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
            return RAGQuery.create_qa_chain()
        
        vectorstore = Chroma(
            persist_directory=maternal_db_dir,
            embedding_function=OpenAIEmbeddings(
                model="text-embedding-3-small"
            ),
            collection_name="maternal_health_knowledge"
        )
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        print(f"ì‚°ëª¨ ê±´ê°• ì»¬ë ‰ì…˜ ë‚´ ë¬¸ì„œ ìˆ˜: {vectorstore._collection.count()}")
        
        retriever = vectorstore.as_retriever(
            search_kwargs={"k": 5},  # ìƒìœ„ 5ê°œ ê²°ê³¼
        )
        
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.5  # ì˜ë£Œ ì •ë³´ëŠ” ë” ì •í™•í•˜ê²Œ
        )

        # ì‚°ëª¨ ê±´ê°• ê´€ë ¨ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        template = """ë‹¹ì‹ ì€ ì‚°ëª¨ë¥¼ ìœ„í•œ ì¹œì ˆí•˜ê³  ê³µê°ì ì¸ ê±´ê°• ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ëª¨ë“  ì§ˆë¬¸ì€ ì‚°ëª¨ê°€ í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì‘ë‹µí•´ì£¼ì„¸ìš”.
        
        ë‹¤ìŒì€ ì‚°ëª¨ ê±´ê°• ê´€ë ¨ ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ê²€ìƒ‰ëœ ì •ë³´ì…ë‹ˆë‹¤:
        {retrieved_context}
        
        ì‚°ëª¨ì˜ ì§ˆë¬¸:
        {question}
        
        ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
        1. ì‚°ëª¨ì˜ ì…ì¥ì—ì„œ ê³µê°í•˜ê³  ì§€ì§€í•˜ëŠ” íƒœë„ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
        2. ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ë˜, ì˜í•™ì  ì •ë³´ëŠ” ì‹ ì¤‘í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
        3. ì‚°ëª¨ì˜ ë¶ˆì•ˆì„ ì¤„ì´ê³  ì•ˆì‹¬ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ì‚¬ì‹¤ì— ê¸°ë°˜í•´ì•¼ í•©ë‹ˆë‹¤.
        4. í•„ìš”í•œ ê²½ìš° ì˜ì‚¬ì™€ ìƒë‹´í•  ê²ƒì„ ê¶Œìœ í•˜ì„¸ìš”.
        5. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ë”°ëœ»í•œ ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        6. ì‚°ëª¨ê°€ ê²ªì„ ìˆ˜ ìˆëŠ” ê°ì •ì  ì–´ë ¤ì›€ì— ê³µê°í•˜ê³ , ì •ì„œì  ì§€ì›ì„ ì œê³µí•˜ì„¸ìš”.
        7. "~ë‹˜"ê³¼ ê°™ì€ ì¡´ì¹­ì„ ì‚¬ìš©í•˜ì—¬ ì¡´ì¤‘í•˜ëŠ” íƒœë„ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”.
        8. ì‚°ëª¨ì˜ ê±´ê°•ê³¼ ì•„ê¸°ì˜ ê±´ê°• ëª¨ë‘ë¥¼ ê³ ë ¤í•œ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm
        
        return retriever, chain

    @staticmethod
    def get_answer(question: str):
        # ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì¶”ê°€ì ì¸ ë¬¸ì„œ(ëŒ€í™” ê´€ë ¨ ë¬¸ë§¥) ê°€ì ¸ì˜¤ê¸°
        retriever, chain = RAGQuery.create_qa_chain()
        retrieved_docs = retriever.invoke(question)
        retrieved_context = "\n".join([doc.page_content for doc in retrieved_docs])
        print(f"Retrieved Context: {retrieved_context}")
        result = chain.invoke({
            "retrieved_context": retrieved_context,
            "question": question
        })
        return result.content
        
    @staticmethod
    def get_maternal_health_answer(question: str):
        """ì‚°ëª¨ ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        # ëª¨ë“  ì§ˆë¬¸ì„ ì‚°ëª¨ ê±´ê°• ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
        # ì‚°ëª¨ ê±´ê°• QA ì²´ì¸ ì‚¬ìš©
        retriever, chain = RAGQuery.create_maternal_health_qa_chain()
        
        retrieved_docs = retriever.invoke(question)
        retrieved_context = "\n".join([doc.page_content for doc in retrieved_docs])

        print(f"Retrieved Context: {retrieved_context}")
        result = chain.invoke({
            "retrieved_context": retrieved_context,
            "question": question
        })
        return result.content